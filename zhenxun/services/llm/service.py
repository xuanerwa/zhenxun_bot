"""
LLM æ¨¡å‹å®ç°ç±»

åŒ…å« LLM æ¨¡å‹çš„æŠ½è±¡åŸºç±»å’Œå…·ä½“å®ç°ï¼Œè´Ÿè´£ä¸å„ç§ AI æä¾›å•†çš„ API äº¤äº’ã€‚
"""

from abc import ABC, abstractmethod
import asyncio
from collections.abc import Awaitable, Callable
import json
import re
import time
from typing import Any, Literal, TypeVar, cast

import httpx
from pydantic import BaseModel, ConfigDict, Field

from zhenxun.services.log import logger
from zhenxun.utils.http_utils import AsyncHttpx
from zhenxun.utils.log_sanitizer import sanitize_for_logging
from zhenxun.utils.pydantic_compat import dump_json_safely

from .adapters.base import BaseAdapter, RequestData, process_image_data
from .config import LLMGenerationConfig
from .config.generation import LLMEmbeddingConfig
from .config.providers import get_llm_config
from .core import (
    KeyStatusStore,
    LLMHttpClient,
    RetryConfig,
    _should_retry_llm_error,
    http_client_manager,
)
from .types import (
    LLMErrorCode,
    LLMException,
    LLMMessage,
    LLMResponse,
    LLMToolCall,
    ModelDetail,
    ProviderConfig,
    ToolChoice,
)
from .types.capabilities import ModelCapabilities, ModelModality

T = TypeVar("T", bound=BaseModel)


class LLMContext(BaseModel):
    """LLM æ‰§è¡Œä¸Šä¸‹æ–‡ï¼Œç”¨äºåœ¨ä¸­é—´ä»¶ç®¡é“ä¸­ä¼ é€’è¯·æ±‚çŠ¶æ€"""

    messages: list[LLMMessage]
    config: LLMGenerationConfig | LLMEmbeddingConfig
    tools: list[Any] | None
    tool_choice: str | dict[str, Any] | ToolChoice | None
    timeout: float | None
    extra: dict[str, Any] = Field(default_factory=dict)
    request_type: Literal["generation", "embedding"] = "generation"
    runtime_state: dict[str, Any] = Field(
        default_factory=dict,
        description="ä¸­é—´ä»¶è¿è¡Œæ—¶çš„ä¸´æ—¶çŠ¶æ€å­˜å‚¨(api_key, retry_countç­‰)",
    )

    model_config = ConfigDict(arbitrary_types_allowed=True)


NextCall = Callable[[LLMContext], Awaitable[LLMResponse]]
LLMMiddleware = Callable[[LLMContext, NextCall], Awaitable[LLMResponse]]


class BaseLLMMiddleware(ABC):
    """LLM ä¸­é—´ä»¶æŠ½è±¡åŸºç±»"""

    @abstractmethod
    async def __call__(self, context: LLMContext, next_call: NextCall) -> LLMResponse:
        """
        æ‰§è¡Œä¸­é—´ä»¶é€»è¾‘

        Args:
            context: è¯·æ±‚ä¸Šä¸‹æ–‡ï¼ŒåŒ…å«é…ç½®å’Œè¿è¡Œæ—¶çŠ¶æ€
            next_call: è°ƒç”¨é“¾ä¸­çš„ä¸‹ä¸€ä¸ªå¤„ç†å‡½æ•°

        Returns:
            LLMResponse: æ¨¡å‹å“åº”ç»“æœ
        """
        pass


class LLMModelBase(ABC):
    """LLMæ¨¡å‹æŠ½è±¡åŸºç±»"""

    @abstractmethod
    async def generate_response(
        self,
        messages: list[LLMMessage],
        config: LLMGenerationConfig | None = None,
        tools: list[Any] | None = None,
        tool_choice: str | dict[str, Any] | ToolChoice | None = None,
        timeout: float | None = None,
    ) -> LLMResponse:
        """ç”Ÿæˆé«˜çº§å“åº”"""
        pass

    @abstractmethod
    async def generate_embeddings(
        self,
        texts: list[str],
        config: LLMEmbeddingConfig,
    ) -> list[list[float]]:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡"""
        pass


class LLMModel(LLMModelBase):
    """LLM æ¨¡å‹å®ç°ç±»"""

    def __init__(
        self,
        provider_config: ProviderConfig,
        model_detail: ModelDetail,
        key_store: KeyStatusStore,
        http_client: LLMHttpClient,
        capabilities: ModelCapabilities,
        config_override: LLMGenerationConfig | None = None,
    ):
        self.provider_config = provider_config
        self.model_detail = model_detail
        self.key_store = key_store
        self.http_client: LLMHttpClient = http_client
        self.capabilities = capabilities
        self._generation_config = config_override

        self.provider_name = provider_config.name
        self.api_type = provider_config.api_type
        self.api_base = provider_config.api_base
        self.api_keys = (
            [provider_config.api_key]
            if isinstance(provider_config.api_key, str)
            else provider_config.api_key
        )
        self.model_name = model_detail.model_name
        self.temperature = model_detail.temperature
        self.max_tokens = model_detail.max_tokens

        self._is_closed = False
        self._ref_count = 0
        self._middlewares: list[LLMMiddleware] = []

    def _has_modality(self, modality: ModelModality, is_input: bool = True) -> bool:
        target_set = (
            self.capabilities.input_modalities
            if is_input
            else self.capabilities.output_modalities
        )
        return modality in target_set

    @property
    def can_process_images(self) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒå›¾ç‰‡ä½œä¸ºè¾“å…¥ã€‚"""
        return self._has_modality(ModelModality.IMAGE)

    @property
    def can_process_video(self) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒè§†é¢‘ä½œä¸ºè¾“å…¥ã€‚"""
        return self._has_modality(ModelModality.VIDEO)

    @property
    def can_process_audio(self) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒéŸ³é¢‘ä½œä¸ºè¾“å…¥ã€‚"""
        return self._has_modality(ModelModality.AUDIO)

    @property
    def can_generate_images(self) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒç”Ÿæˆå›¾ç‰‡ã€‚"""
        return self._has_modality(ModelModality.IMAGE, is_input=False)

    @property
    def can_generate_audio(self) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒç”ŸæˆéŸ³é¢‘ (TTS)ã€‚"""
        return self._has_modality(ModelModality.AUDIO, is_input=False)

    @property
    def is_embedding_model(self) -> bool:
        """æ£€æŸ¥è¿™æ˜¯å¦æ˜¯ä¸€ä¸ªåµŒå…¥æ¨¡å‹ã€‚"""
        return self.capabilities.is_embedding_model

    def add_middleware(self, middleware: LLMMiddleware) -> None:
        """æ³¨å†Œä¸€ä¸ªä¸­é—´ä»¶åˆ°å¤„ç†ç®¡é“çš„æœ€å¤–å±‚"""
        self._middlewares.append(middleware)

    def _build_pipeline(self) -> NextCall:
        """
        æ„å»ºå®Œæ•´çš„ä¸­é—´ä»¶è°ƒç”¨é“¾ã€‚é¡ºåºä¸ºï¼š
        ç”¨æˆ·è‡ªå®šä¹‰ä¸­é—´ä»¶ -> Retry -> Logging -> KeySelection -> Network (ç»ˆç»“è€…)
        """
        from .adapters import get_adapter_for_api_type

        client_settings = get_llm_config().client_settings
        retry_config = RetryConfig(
            max_retries=client_settings.max_retries,
            retry_delay=client_settings.retry_delay,
        )
        adapter = get_adapter_for_api_type(self.api_type)

        network_middleware = NetworkRequestMiddleware(self, adapter)

        async def terminal_handler(ctx: LLMContext) -> LLMResponse:
            async def _noop(_: LLMContext) -> LLMResponse:
                raise RuntimeError("NetworkRequestMiddleware ä¸åº”è°ƒç”¨ next_call")

            return await network_middleware(ctx, _noop)

        def _wrap(middleware: LLMMiddleware, next_call: NextCall) -> NextCall:
            async def _handler(inner_ctx: LLMContext) -> LLMResponse:
                return await middleware(inner_ctx, next_call)

            return _handler

        handler: NextCall = terminal_handler
        handler = _wrap(
            KeySelectionMiddleware(self.key_store, self.provider_name, self.api_keys),
            handler,
        )
        handler = _wrap(
            LoggingMiddleware(self.provider_name, self.model_name),
            handler,
        )
        handler = _wrap(
            RetryMiddleware(retry_config, self.key_store),
            handler,
        )

        for middleware in reversed(self._middlewares):
            handler = _wrap(middleware, handler)

        return handler

    def _get_effective_api_type(self) -> str:
        """
        è·å–å®é™…ç”Ÿæ•ˆçš„ API ç±»å‹ã€‚
        ä¸»è¦ç”¨äº Smart æ¨¡å¼ä¸‹ï¼Œåˆ¤æ–­æ—¥å¿—å‡€åŒ–åº”è¯¥ä½¿ç”¨å“ªç§æ ¼å¼ã€‚
        """
        if self.api_type != "smart":
            return self.api_type

        if self.model_detail.api_type:
            return self.model_detail.api_type
        if (
            "gemini" in self.model_name.lower()
            and "openai" not in self.model_name.lower()
        ):
            return "gemini"
        return "openai"

    async def _get_http_client(self) -> LLMHttpClient:
        """è·å–HTTPå®¢æˆ·ç«¯"""
        if self.http_client.is_closed:
            logger.debug(
                f"LLMModel {self.provider_name}/{self.model_name} çš„ HTTP å®¢æˆ·ç«¯å·²å…³é—­,"
                "æ­£åœ¨è·å–æ–°çš„å®¢æˆ·ç«¯"
            )
            self.http_client = await http_client_manager.get_client(
                self.provider_config
            )
        return self.http_client

    async def _select_api_key(self, failed_keys: set[str] | None = None) -> str:
        """é€‰æ‹©å¯ç”¨çš„APIå¯†é’¥ï¼ˆä½¿ç”¨è½®è¯¢ç­–ç•¥ï¼‰"""
        if not self.api_keys:
            raise LLMException(
                f"æä¾›å•† {self.provider_name} æ²¡æœ‰é…ç½®APIå¯†é’¥",
                code=LLMErrorCode.NO_AVAILABLE_KEYS,
            )

        selected_key = await self.key_store.get_next_available_key(
            self.provider_name, self.api_keys, failed_keys
        )

        if not selected_key:
            raise LLMException(
                f"æä¾›å•† {self.provider_name} çš„æ‰€æœ‰APIå¯†é’¥å½“å‰éƒ½ä¸å¯ç”¨",
                code=LLMErrorCode.NO_AVAILABLE_KEYS,
                details={
                    "total_keys": len(self.api_keys),
                    "failed_keys": len(failed_keys or set()),
                },
            )

        return selected_key

    async def close(self):
        """æ ‡è®°æ¨¡å‹å®ä¾‹çš„å½“å‰ä½¿ç”¨å‘¨æœŸç»“æŸ"""
        if self._is_closed:
            return
        self._is_closed = True
        logger.debug(
            f"LLMModelå®ä¾‹çš„ä½¿ç”¨å‘¨æœŸå·²ç»“æŸ: {self} (å…±äº«HTTPå®¢æˆ·ç«¯çŠ¶æ€ä¸å—å½±å“)"
        )

    async def __aenter__(self):
        if self._is_closed:
            logger.debug(
                f"Re-entering context for closed LLMModel {self}. "
                f"Resetting _is_closed to False."
            )
            self._is_closed = False
        self._check_not_closed()
        self._ref_count += 1
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        _ = exc_type, exc_val, exc_tb
        self._ref_count -= 1
        if self._ref_count <= 0:
            self._ref_count = 0
            await self.close()

    def _check_not_closed(self):
        """æ£€æŸ¥å®ä¾‹æ˜¯å¦å·²å…³é—­"""
        if self._is_closed:
            raise RuntimeError(f"LLMModelå®ä¾‹å·²å…³é—­: {self}")

    async def _execute_core_generation(self, context: LLMContext) -> LLMResponse:
        """
        [å†…æ ¸] æ‰§è¡Œæ ¸å¿ƒç”Ÿæˆé€»è¾‘ï¼šæ„å»ºç®¡é“å¹¶æ‰§è¡Œã€‚
        æ­¤æ–¹æ³•ä½œä¸ºä¸­é—´ä»¶ç®¡é“çš„ç»ˆç‚¹è¢«è°ƒç”¨ã€‚
        """
        pipeline_handler = self._build_pipeline()
        return await pipeline_handler(context)

    async def generate_response(
        self,
        messages: list[LLMMessage],
        config: LLMGenerationConfig | None = None,
        tools: list[Any] | None = None,
        tool_choice: str | dict[str, Any] | ToolChoice | None = None,
        timeout: float | None = None,
    ) -> LLMResponse:
        """
        ç”Ÿæˆé«˜çº§å“åº” (æ”¯æŒä¸­é—´ä»¶ç®¡é“)ã€‚
        """
        self._check_not_closed()

        if self._generation_config and config:
            final_request_config = self._generation_config.merge_with(config)
        elif config:
            final_request_config = config
        else:
            final_request_config = self._generation_config or LLMGenerationConfig()

        normalized_tools: list[Any] | None = None
        if tools:
            if isinstance(tools, dict):
                normalized_tools = list(tools.values())
            elif isinstance(tools, list):
                normalized_tools = tools
            else:
                normalized_tools = [tools]

        context = LLMContext(
            messages=messages,
            config=final_request_config,
            tools=normalized_tools,
            tool_choice=tool_choice,
            timeout=timeout,
        )

        return await self._execute_core_generation(context)

    async def generate_embeddings(
        self,
        texts: list[str],
        config: LLMEmbeddingConfig | None = None,
    ) -> list[list[float]]:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡"""
        self._check_not_closed()
        if not texts:
            return []

        final_config = config or LLMEmbeddingConfig()

        context = LLMContext(
            messages=[],
            config=final_config,
            tools=None,
            tool_choice=None,
            timeout=None,
            request_type="embedding",
            extra={"texts": texts},
        )

        pipeline = self._build_pipeline()
        response = await pipeline(context)
        embeddings = (
            response.cache_info.get("embeddings") if response.cache_info else None
        )
        if embeddings is None:
            raise LLMException(
                "åµŒå…¥è¯·æ±‚æœªè¿”å› embeddings æ•°æ®",
                code=LLMErrorCode.EMBEDDING_FAILED,
            )
        return embeddings

    def __str__(self) -> str:
        status = "closed" if self._is_closed else "active"
        return f"LLMModel({self.provider_name}/{self.model_name}, {status})"

    def __repr__(self) -> str:
        status = "closed" if self._is_closed else "active"
        return (
            f"LLMModel(provider={self.provider_name}, model={self.model_name}, "
            f"api_type={self.api_type}, status={status})"
        )


class RetryMiddleware(BaseLLMMiddleware):
    """
    é‡è¯•ä¸­é—´ä»¶ï¼šå¤„ç†å¼‚å¸¸æ•è·ä¸é‡è¯•å¾ªç¯
    """

    def __init__(self, retry_config: RetryConfig, key_store: KeyStatusStore):
        self.retry_config = retry_config
        self.key_store = key_store

    async def __call__(self, context: LLMContext, next_call: NextCall) -> LLMResponse:
        last_exception: Exception | None = None
        total_attempts = self.retry_config.max_retries + 1

        for attempt in range(total_attempts):
            try:
                context.runtime_state["attempt"] = attempt + 1
                return await next_call(context)

            except LLMException as e:
                last_exception = e
                api_key = context.runtime_state.get("api_key")

                if api_key:
                    status_code = e.details.get("status_code")
                    error_msg = f"({e.code.name}) {e.message}"
                    await self.key_store.record_failure(api_key, status_code, error_msg)

                if not _should_retry_llm_error(
                    e, attempt, self.retry_config.max_retries
                ):
                    raise e

                if attempt == total_attempts - 1:
                    raise e

                wait_time = self.retry_config.retry_delay
                if self.retry_config.exponential_backoff:
                    wait_time *= 2**attempt

                logger.warning(
                    f"è¯·æ±‚å¤±è´¥ï¼Œ{wait_time:.2f}ç§’åé‡è¯•"
                    f" (ç¬¬{attempt + 1}/{self.retry_config.max_retries}æ¬¡é‡è¯•): {e}"
                )
                await asyncio.sleep(wait_time)

            except Exception as e:
                logger.error(f"éé¢„æœŸå¼‚å¸¸ï¼Œåœæ­¢é‡è¯•: {e}", e=e)
                raise e

        if last_exception:
            raise last_exception
        raise LLMException("é‡è¯•å¾ªç¯å¼‚å¸¸ç»“æŸ")


class KeySelectionMiddleware(BaseLLMMiddleware):
    """
    å¯†é’¥é€‰æ‹©ä¸­é—´ä»¶ï¼šè´Ÿè´£è½®è¯¢è·å–å¯ç”¨ API Key
    """

    def __init__(
        self, key_store: KeyStatusStore, provider_name: str, api_keys: list[str]
    ):
        self.key_store = key_store
        self.provider_name = provider_name
        self.api_keys = api_keys
        self._failed_keys: set[str] = set()

    async def __call__(self, context: LLMContext, next_call: NextCall) -> LLMResponse:
        selected_key = await self.key_store.get_next_available_key(
            self.provider_name, self.api_keys, exclude_keys=self._failed_keys
        )

        if not selected_key:
            raise LLMException(
                f"æä¾›å•† {self.provider_name} æ— å¯ç”¨ API Key",
                code=LLMErrorCode.NO_AVAILABLE_KEYS,
            )

        context.runtime_state["api_key"] = selected_key

        try:
            response = await next_call(context)
            return response
        except LLMException as e:
            self._failed_keys.add(selected_key)
            masked = f"{selected_key[:8]}..."
            if isinstance(e.details, dict):
                e.details["api_key"] = masked
            raise e


class LoggingMiddleware(BaseLLMMiddleware):
    """
    æ—¥å¿—ä¸­é—´ä»¶ï¼šè´Ÿè´£è¯·æ±‚å’Œå“åº”çš„æ—¥å¿—è®°å½•ä¸è„±æ•
    """

    def __init__(
        self, provider_name: str, model_name: str, log_context: str = "Generation"
    ):
        self.provider_name = provider_name
        self.model_name = model_name
        self.log_context = log_context

    async def __call__(self, context: LLMContext, next_call: NextCall) -> LLMResponse:
        attempt = context.runtime_state.get("attempt", 1)
        api_key = context.runtime_state.get("api_key", "unknown")
        masked_key = f"{api_key[:8]}..."

        logger.info(
            f"ğŸŒ å‘èµ·LLMè¯·æ±‚ (å°è¯• {attempt}) - {self.provider_name}/{self.model_name} "
            f"[{self.log_context}] Key: {masked_key}"
        )

        try:
            start_time = time.monotonic()
            response = await next_call(context)
            duration = (time.monotonic() - start_time) * 1000
            logger.info(f"ğŸ¯ LLMå“åº”æˆåŠŸ [{self.log_context}] è€—æ—¶: {duration:.2f}ms")
            return response
        except Exception as e:
            logger.error(f"âŒ è¯·æ±‚å¼‚å¸¸ [{self.log_context}]: {type(e).__name__} - {e}")
            raise e


class NetworkRequestMiddleware(BaseLLMMiddleware):
    """
    ç½‘ç»œè¯·æ±‚ä¸­é—´ä»¶ï¼šæ‰§è¡Œ Adapter è½¬æ¢å’Œ HTTP è¯·æ±‚
    """

    def __init__(self, model_instance: "LLMModel", adapter: "BaseAdapter"):
        self.model = model_instance
        self.http_client = model_instance.http_client
        self.adapter = adapter
        self.key_store = model_instance.key_store

    async def __call__(self, context: LLMContext, next_call: NextCall) -> LLMResponse:
        api_key = context.runtime_state["api_key"]

        request_data: RequestData
        gen_config: LLMGenerationConfig | None = None
        embed_config: LLMEmbeddingConfig | None = None

        if context.request_type == "embedding":
            embed_config = cast(LLMEmbeddingConfig, context.config)
            texts = (context.extra or {}).get("texts", [])
            request_data = self.adapter.prepare_embedding_request(
                model=self.model,
                api_key=api_key,
                texts=texts,
                config=embed_config,
            )
        else:
            gen_config = cast(LLMGenerationConfig, context.config)
            request_data = await self.adapter.prepare_advanced_request(
                model=self.model,
                api_key=api_key,
                messages=context.messages,
                config=gen_config,
                tools=context.tools,
                tool_choice=context.tool_choice,
            )

        masked_key = (
            f"{api_key[:8]}...{api_key[-4:] if len(api_key) > 12 else '***'}"
            if api_key
            else "N/A"
        )
        logger.debug(f"ğŸ”‘ APIå¯†é’¥: {masked_key}")
        logger.debug(f"ğŸ“¡ è¯·æ±‚URL: {request_data.url}")
        logger.debug(f"ğŸ“‹ è¯·æ±‚å¤´: {dict(request_data.headers)}")

        if self.model.api_type == "smart":
            effective_type = self.model._get_effective_api_type()
            sanitizer_req_context = f"{effective_type}_request"
        else:
            sanitizer_req_context = self.adapter.log_sanitization_context
        sanitized_body = sanitize_for_logging(
            request_data.body, context=sanitizer_req_context
        )

        if request_data.files and isinstance(sanitized_body, dict):
            file_info: list[str] = []
            file_count = 0
            if isinstance(request_data.files, list):
                file_count = len(request_data.files)
                for key, value in request_data.files:
                    filename = (
                        value[0]
                        if isinstance(value, tuple) and len(value) > 0
                        else "..."
                    )
                    file_info.append(f"{key}='{filename}'")
            elif isinstance(request_data.files, dict):
                file_count = len(request_data.files)
                file_info = list(request_data.files.keys())

            sanitized_body["[MULTIPART_FILES]"] = f"Count: {file_count} | {file_info}"

        request_body_str = dump_json_safely(
            sanitized_body, ensure_ascii=False, indent=2
        )
        logger.debug(f"ğŸ“¦ è¯·æ±‚ä½“: {request_body_str}")

        start_time = time.monotonic()
        try:
            http_response = await self.http_client.post(
                request_data.url,
                headers=request_data.headers,
                content=dump_json_safely(request_data.body, ensure_ascii=False)
                if not request_data.files
                else None,
                data=request_data.body if request_data.files else None,
                files=request_data.files,
                timeout=context.timeout,
            )

            logger.debug(f"ğŸ“¥ å“åº”çŠ¶æ€ç : {http_response.status_code}")

            if exception := self.adapter.handle_http_error(http_response):
                error_text = http_response.content.decode("utf-8", errors="ignore")
                logger.debug(f"ğŸ’¥ å®Œæ•´é”™è¯¯å“åº”: {error_text}")
                await self.key_store.record_failure(
                    api_key, http_response.status_code, error_text
                )
                raise exception

            response_bytes = await http_response.aread()
            logger.debug(f"ğŸ“¦ å“åº”ä½“å·²å®Œæ•´è¯»å– ({len(response_bytes)} bytes)")

            response_json = json.loads(response_bytes)

            sanitizer_resp_context = sanitizer_req_context.replace(
                "_request", "_response"
            )
            if sanitizer_resp_context == sanitizer_req_context:
                sanitizer_resp_context = f"{sanitizer_req_context}_response"

            sanitized_response = sanitize_for_logging(
                response_json, context=sanitizer_resp_context
            )
            response_json_str = json.dumps(
                sanitized_response, ensure_ascii=False, indent=2
            )
            logger.debug(f"ğŸ“‹ å“åº”JSON: {response_json_str}")

            if context.request_type == "embedding":
                self.adapter.validate_embedding_response(response_json)
                embeddings = self.adapter.parse_embedding_response(response_json)
                latency = (time.monotonic() - start_time) * 1000
                await self.key_store.record_success(api_key, latency)

                return LLMResponse(
                    text="",
                    raw_response=response_json,
                    cache_info={"embeddings": embeddings},
                )

            response_data = self.adapter.parse_response(
                self.model, response_json, is_advanced=True
            )

            should_rescue_image = (
                gen_config
                and gen_config.validation_policy
                and gen_config.validation_policy.get("require_image")
            )
            if (
                should_rescue_image
                and not response_data.images
                and response_data.text
                and gen_config
            ):
                markdown_matches = re.findall(
                    r"(!?\[.*?\]\((https?://[^\)]+)\))", response_data.text
                )
                if markdown_matches:
                    logger.info(
                        f"æ£€æµ‹åˆ° {len(markdown_matches)} "
                        "ä¸ªèµ„æºé“¾æ¥ï¼Œå°è¯•è‡ªåŠ¨ä¸‹è½½å¹¶æ¸…æ´—ã€‚"
                    )
                    if response_data.images is None:
                        response_data.images = []

                    downloaded_urls = set()
                    for full_tag, url in markdown_matches:
                        try:
                            if url not in downloaded_urls:
                                content = await AsyncHttpx.get_content(url)
                                response_data.images.append(process_image_data(content))
                                downloaded_urls.add(url)
                            response_data.text = response_data.text.replace(
                                full_tag, ""
                            )
                        except Exception as exc:
                            logger.warning(
                                f"è‡ªåŠ¨ä¸‹è½½ç”Ÿæˆçš„å›¾ç‰‡å¤±è´¥: {url}, é”™è¯¯: {exc}"
                            )
                    response_data.text = response_data.text.strip()

            latency = (time.monotonic() - start_time) * 1000
            await self.key_store.record_success(api_key, latency)

            response_tool_calls: list[LLMToolCall] = []
            if response_data.tool_calls:
                for tc_data in response_data.tool_calls:
                    if isinstance(tc_data, LLMToolCall):
                        response_tool_calls.append(tc_data)
                    elif isinstance(tc_data, dict):
                        try:
                            response_tool_calls.append(LLMToolCall(**tc_data))
                        except Exception:
                            pass

            final_response = LLMResponse(
                text=response_data.text,
                content_parts=response_data.content_parts,
                usage_info=response_data.usage_info,
                images=response_data.images,
                raw_response=response_data.raw_response,
                tool_calls=response_tool_calls if response_tool_calls else None,
                code_executions=response_data.code_executions,
                grounding_metadata=response_data.grounding_metadata,
                cache_info=response_data.cache_info,
                thought_text=response_data.thought_text,
                thought_signature=response_data.thought_signature,
            )

            if context.request_type == "generation" and gen_config:
                if gen_config.response_validator:
                    try:
                        gen_config.response_validator(final_response)
                    except Exception as exc:
                        raise LLMException(
                            f"å“åº”å†…å®¹æœªé€šè¿‡è‡ªå®šä¹‰éªŒè¯å™¨: {exc}",
                            code=LLMErrorCode.API_RESPONSE_INVALID,
                            details={"validator_error": str(exc)},
                            cause=exc,
                        ) from exc

                policy = gen_config.validation_policy
                if policy:
                    effective_type = self.model._get_effective_api_type()
                    if policy.get("require_image") and not final_response.images:
                        if effective_type == "gemini" and response_data.raw_response:
                            usage_metadata = response_data.raw_response.get(
                                "usageMetadata", {}
                            )
                            prompt_token_details = usage_metadata.get(
                                "promptTokensDetails", []
                            )
                            prompt_had_image = any(
                                detail.get("modality") == "IMAGE"
                                for detail in prompt_token_details
                            )

                            if prompt_had_image:
                                raise LLMException(
                                    "å“åº”éªŒè¯å¤±è´¥ï¼šæ¨¡å‹æ¥æ”¶äº†å›¾ç‰‡è¾“å…¥ä½†æœªç”Ÿæˆå›¾ç‰‡ã€‚",
                                    code=LLMErrorCode.API_RESPONSE_INVALID,
                                    details={
                                        "policy": policy,
                                        "text_response": final_response.text,
                                        "raw_response": response_data.raw_response,
                                    },
                                )
                            else:
                                logger.debug(
                                    "Geminiæç¤ºè¯ä¸­æœªåŒ…å«å›¾ç‰‡ï¼Œè·³è¿‡å›¾ç‰‡è¦æ±‚é‡è¯•ã€‚"
                                )
                        else:
                            raise LLMException(
                                "å“åº”éªŒè¯å¤±è´¥ï¼šè¦æ±‚è¿”å›å›¾ç‰‡ä½†æœªæ‰¾åˆ°å›¾ç‰‡æ•°æ®ã€‚",
                                code=LLMErrorCode.API_RESPONSE_INVALID,
                                details={
                                    "policy": policy,
                                    "text_response": final_response.text,
                                },
                            )

            return final_response

        except Exception as e:
            if isinstance(e, LLMException):
                raise e

            logger.error(f"è§£æå“åº”å¤±è´¥æˆ–å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

            if not isinstance(e, httpx.NetworkError | httpx.TimeoutException):
                await self.key_store.record_failure(api_key, None, str(e))

            raise LLMException(
                f"ç½‘ç»œè¯·æ±‚å¼‚å¸¸: {type(e).__name__} - {e}",
                code=LLMErrorCode.API_REQUEST_FAILED,
                details={"api_key": masked_key},
                cause=e,
            )
