"""
LLM è½»é‡çº§å·¥å…·æ‰§è¡Œå™¨

æä¾›é©±åŠ¨ LLM ä¸æœ¬åœ°å‡½æ•°å·¥å…·ä¹‹é—´äº¤äº’çš„æ ¸å¿ƒå¾ªç¯ã€‚
"""

import asyncio
from enum import Enum
import json
from typing import Any

from pydantic import BaseModel, Field

from zhenxun.services.log import logger
from zhenxun.utils.decorator.retry import Retry
from zhenxun.utils.pydantic_compat import model_dump

from .service import LLMModel
from .types import (
    LLMErrorCode,
    LLMException,
    LLMMessage,
    ToolExecutable,
    ToolResult,
)


class ExecutionConfig(BaseModel):
    """
    è½»é‡çº§æ‰§è¡Œå™¨çš„é…ç½®ã€‚
    """

    max_cycles: int = Field(default=5, description="å·¥å…·è°ƒç”¨å¾ªç¯çš„æœ€å¤§æ¬¡æ•°ã€‚")


class ToolErrorType(str, Enum):
    """ç»“æ„åŒ–å·¥å…·é”™è¯¯çš„ç±»å‹æšä¸¾ã€‚"""

    TOOL_NOT_FOUND = "ToolNotFound"
    INVALID_ARGUMENTS = "InvalidArguments"
    EXECUTION_ERROR = "ExecutionError"
    USER_CANCELLATION = "UserCancellation"


class ToolErrorResult(BaseModel):
    """ä¸€ä¸ªç»“æ„åŒ–çš„å·¥å…·æ‰§è¡Œé”™è¯¯æ¨¡å‹ï¼Œç”¨äºè¿”å›ç»™ LLMã€‚"""

    error_type: ToolErrorType = Field(..., description="é”™è¯¯çš„ç±»å‹ã€‚")
    message: str = Field(..., description="å¯¹é”™è¯¯çš„è¯¦ç»†æè¿°ã€‚")
    is_retryable: bool = Field(False, description="æŒ‡ç¤ºè¿™ä¸ªé”™è¯¯æ˜¯å¦å¯èƒ½é€šè¿‡é‡è¯•è§£å†³ã€‚")

    def model_dump(self, **kwargs):
        return model_dump(self, **kwargs)


def _is_exception_retryable(e: Exception) -> bool:
    """åˆ¤æ–­ä¸€ä¸ªå¼‚å¸¸æ˜¯å¦åº”è¯¥è§¦å‘é‡è¯•ã€‚"""
    if isinstance(e, LLMException):
        retryable_codes = {
            LLMErrorCode.API_REQUEST_FAILED,
            LLMErrorCode.API_TIMEOUT,
            LLMErrorCode.API_RATE_LIMITED,
        }
        return e.code in retryable_codes
    return True


class LLMToolExecutor:
    """
    ä¸€ä¸ªé€šç”¨çš„æ‰§è¡Œå™¨ï¼Œè´Ÿè´£é©±åŠ¨ LLM ä¸å·¥å…·ä¹‹é—´çš„å¤šè½®äº¤äº’ã€‚
    """

    def __init__(self, model: LLMModel):
        self.model = model

    async def run(
        self,
        messages: list[LLMMessage],
        tools: dict[str, ToolExecutable],
        config: ExecutionConfig | None = None,
    ) -> list[LLMMessage]:
        """
        æ‰§è¡Œå®Œæ•´çš„æ€è€ƒ-è¡ŒåŠ¨å¾ªç¯ã€‚
        """
        effective_config = config or ExecutionConfig()
        execution_history = list(messages)

        for i in range(effective_config.max_cycles):
            response = await self.model.generate_response(
                execution_history, tools=tools
            )

            assistant_message = LLMMessage(
                role="assistant",
                content=response.text,
                tool_calls=response.tool_calls,
            )
            execution_history.append(assistant_message)

            if not response.tool_calls:
                logger.info("âœ… LLMToolExecutorï¼šæ¨¡å‹æœªè¯·æ±‚å·¥å…·è°ƒç”¨ï¼Œæ‰§è¡Œç»“æŸã€‚")
                return execution_history

            logger.info(
                f"ğŸ› ï¸ LLMToolExecutorï¼šæ¨¡å‹è¯·æ±‚å¹¶è¡Œè°ƒç”¨ {len(response.tool_calls)} ä¸ªå·¥å…·"
            )
            tool_results = await self._execute_tools_parallel_safely(
                response.tool_calls,
                tools,
            )
            execution_history.extend(tool_results)

        raise LLMException(
            f"è¶…è¿‡æœ€å¤§å·¥å…·è°ƒç”¨å¾ªç¯æ¬¡æ•° ({effective_config.max_cycles})ã€‚",
            code=LLMErrorCode.GENERATION_FAILED,
        )

    async def _execute_single_tool_safely(
        self, tool_call: Any, available_tools: dict[str, ToolExecutable]
    ) -> tuple[Any, ToolResult]:
        """å®‰å…¨åœ°æ‰§è¡Œå•ä¸ªå·¥å…·è°ƒç”¨ã€‚"""
        tool_name = tool_call.function.name
        arguments = {}

        try:
            if tool_call.function.arguments:
                arguments = json.loads(tool_call.function.arguments)
        except json.JSONDecodeError as e:
            error_result = ToolErrorResult(
                error_type=ToolErrorType.INVALID_ARGUMENTS,
                message=f"å‚æ•°è§£æå¤±è´¥: {e}",
                is_retryable=False,
            )
            return tool_call, ToolResult(output=model_dump(error_result))

        try:
            executable = available_tools.get(tool_name)
            if not executable:
                raise LLMException(
                    f"Tool '{tool_name}' not found.",
                    code=LLMErrorCode.CONFIGURATION_ERROR,
                )

            @Retry.simple(
                stop_max_attempt=2, wait_fixed_seconds=1, return_on_failure=None
            )
            async def execute_with_retry():
                return await executable.execute(**arguments)

            execution_result = await execute_with_retry()
            if execution_result is None:
                raise LLMException("å·¥å…·æ‰§è¡Œåœ¨å¤šæ¬¡é‡è¯•åä»ç„¶å¤±è´¥ã€‚")

            return tool_call, execution_result
        except Exception as e:
            error_type = ToolErrorType.EXECUTION_ERROR
            is_retryable = _is_exception_retryable(e)
            if (
                isinstance(e, LLMException)
                and e.code == LLMErrorCode.CONFIGURATION_ERROR
            ):
                error_type = ToolErrorType.TOOL_NOT_FOUND
                is_retryable = False

            error_result = ToolErrorResult(
                error_type=error_type, message=str(e), is_retryable=is_retryable
            )
            return tool_call, ToolResult(output=model_dump(error_result))

    async def _execute_tools_parallel_safely(
        self,
        tool_calls: list[Any],
        available_tools: dict[str, ToolExecutable],
    ) -> list[LLMMessage]:
        """å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨ï¼Œå¹¶å¯¹æ¯ä¸ªè°ƒç”¨çš„é”™è¯¯è¿›è¡Œéš”ç¦»ã€‚"""
        if not tool_calls:
            return []

        tasks = [
            self._execute_single_tool_safely(call, available_tools)
            for call in tool_calls
        ]
        results = await asyncio.gather(*tasks)

        tool_messages = [
            LLMMessage.tool_response(
                tool_call_id=original_call.id,
                function_name=original_call.function.name,
                result=result.output,
            )
            for original_call, result in results
        ]
        return tool_messages
