"""
Gemini API é€‚é…å™¨
"""

from typing import TYPE_CHECKING, Any

from zhenxun.services.log import logger

from ..types.exceptions import LLMErrorCode, LLMException
from ..utils import sanitize_schema_for_llm
from .base import BaseAdapter, RequestData, ResponseData

if TYPE_CHECKING:
    from ..config.generation import LLMGenerationConfig
    from ..service import LLMModel
    from ..types.content import LLMMessage
    from ..types.enums import EmbeddingTaskType
    from ..types.models import LLMToolCall
    from ..types.protocols import ToolExecutable


class GeminiAdapter(BaseAdapter):
    """Gemini API é€‚é…å™¨"""

    @property
    def api_type(self) -> str:
        return "gemini"

    @property
    def supported_api_types(self) -> list[str]:
        return ["gemini"]

    def get_base_headers(self, api_key: str) -> dict[str, str]:
        """è·å–åŸºç¡€è¯·æ±‚å¤´"""
        from zhenxun.utils.user_agent import get_user_agent

        headers = get_user_agent()
        headers.update({"Content-Type": "application/json"})
        headers["x-goog-api-key"] = api_key

        return headers

    async def prepare_advanced_request(
        self,
        model: "LLMModel",
        api_key: str,
        messages: list["LLMMessage"],
        config: "LLMGenerationConfig | None" = None,
        tools: dict[str, "ToolExecutable"] | None = None,
        tool_choice: str | dict[str, Any] | None = None,
    ) -> RequestData:
        """å‡†å¤‡é«˜çº§è¯·æ±‚"""
        effective_config = config if config is not None else model._generation_config

        endpoint = self._get_gemini_endpoint(model, effective_config)
        url = self.get_api_url(model, endpoint)
        headers = self.get_base_headers(api_key)

        gemini_contents: list[dict[str, Any]] = []
        system_instruction_parts: list[dict[str, Any]] | None = None

        for msg in messages:
            current_parts: list[dict[str, Any]] = []
            if msg.role == "system":
                if isinstance(msg.content, str):
                    system_instruction_parts = [{"text": msg.content}]
                elif isinstance(msg.content, list):
                    system_instruction_parts = [
                        await part.convert_for_api_async("gemini")
                        for part in msg.content
                    ]
                continue

            elif msg.role == "user":
                if isinstance(msg.content, str):
                    current_parts.append({"text": msg.content})
                elif isinstance(msg.content, list):
                    for part_obj in msg.content:
                        current_parts.append(
                            await part_obj.convert_for_api_async("gemini")
                        )
                gemini_contents.append({"role": "user", "parts": current_parts})

            elif msg.role == "assistant" or msg.role == "model":
                if isinstance(msg.content, str) and msg.content:
                    current_parts.append({"text": msg.content})
                elif isinstance(msg.content, list):
                    for part_obj in msg.content:
                        current_parts.append(
                            await part_obj.convert_for_api_async("gemini")
                        )

                if msg.tool_calls:
                    import json

                    for call in msg.tool_calls:
                        current_parts.append(
                            {
                                "functionCall": {
                                    "name": call.function.name,
                                    "args": json.loads(call.function.arguments),
                                }
                            }
                        )
                if current_parts:
                    gemini_contents.append({"role": "model", "parts": current_parts})

            elif msg.role == "tool":
                if not msg.name:
                    raise ValueError("Gemini å·¥å…·æ¶ˆæ¯å¿…é¡»åŒ…å« 'name' å­—æ®µï¼ˆå‡½æ•°åï¼‰ã€‚")

                import json

                try:
                    content_str = (
                        msg.content
                        if isinstance(msg.content, str)
                        else str(msg.content)
                    )
                    tool_result_obj = json.loads(content_str)
                except json.JSONDecodeError:
                    content_str = (
                        msg.content
                        if isinstance(msg.content, str)
                        else str(msg.content)
                    )
                    logger.warning(
                        f"å·¥å…· {msg.name} çš„ç»“æœä¸æ˜¯æœ‰æ•ˆçš„ JSON: {content_str}. "
                        f"åŒ…è£…ä¸ºåŸå§‹å­—ç¬¦ä¸²ã€‚"
                    )
                    tool_result_obj = {"raw_output": content_str}

                if isinstance(tool_result_obj, list):
                    logger.debug(
                        f"å·¥å…· '{msg.name}' çš„è¿”å›ç»“æœæ˜¯åˆ—è¡¨ï¼Œ"
                        f"æ­£åœ¨ä¸ºGemini APIåŒ…è£…ä¸ºJSONå¯¹è±¡ã€‚"
                    )
                    final_response_payload = {"result": tool_result_obj}
                elif not isinstance(tool_result_obj, dict):
                    final_response_payload = {"result": tool_result_obj}
                else:
                    final_response_payload = tool_result_obj

                current_parts.append(
                    {
                        "functionResponse": {
                            "name": msg.name,
                            "response": final_response_payload,
                        }
                    }
                )
                gemini_contents.append({"role": "function", "parts": current_parts})

        body: dict[str, Any] = {"contents": gemini_contents}

        if system_instruction_parts:
            body["systemInstruction"] = {"parts": system_instruction_parts}

        all_tools_for_request = []
        if tools:
            import asyncio

            from zhenxun.utils.pydantic_compat import model_dump

            definition_tasks = [
                executable.get_definition() for executable in tools.values()
            ]
            tool_definitions = await asyncio.gather(*definition_tasks)

            function_declarations = []
            for tool_def in tool_definitions:
                tool_def.parameters = sanitize_schema_for_llm(
                    tool_def.parameters, api_type="gemini"
                )
                function_declarations.append(model_dump(tool_def))

            if function_declarations:
                all_tools_for_request.append(
                    {"functionDeclarations": function_declarations}
                )

        if effective_config:
            if getattr(effective_config, "enable_grounding", False):
                has_explicit_gs_tool = any(
                    "googleSearch" in tool_item for tool_item in all_tools_for_request
                )
                if not has_explicit_gs_tool:
                    all_tools_for_request.append({"googleSearch": {}})
                    logger.debug("éšå¼å¯ç”¨ Google Search å·¥å…·è¿›è¡Œä¿¡æ¯æ¥æºå…³è”ã€‚")

            if getattr(effective_config, "enable_code_execution", False):
                has_explicit_ce_tool = any(
                    "codeExecution" in tool_item for tool_item in all_tools_for_request
                )
                if not has_explicit_ce_tool:
                    all_tools_for_request.append({"codeExecution": {}})
                    logger.debug("éšå¼å¯ç”¨ä»£ç æ‰§è¡Œå·¥å…·ã€‚")

        if all_tools_for_request:
            body["tools"] = all_tools_for_request

        final_tool_choice = tool_choice
        if final_tool_choice is None and effective_config:
            final_tool_choice = getattr(effective_config, "tool_choice", None)

        if final_tool_choice:
            if isinstance(final_tool_choice, str):
                mode_upper = final_tool_choice.upper()
                if mode_upper in ["AUTO", "NONE", "ANY"]:
                    body["toolConfig"] = {"functionCallingConfig": {"mode": mode_upper}}
                else:
                    body["toolConfig"] = self._convert_tool_choice_to_gemini(
                        final_tool_choice
                    )
            else:
                body["toolConfig"] = self._convert_tool_choice_to_gemini(
                    final_tool_choice
                )

        final_generation_config = self._build_gemini_generation_config(
            model, effective_config
        )
        if final_generation_config:
            body["generationConfig"] = final_generation_config

        safety_settings = self._build_safety_settings(effective_config)
        if safety_settings:
            body["safetySettings"] = safety_settings

        return RequestData(url=url, headers=headers, body=body)

    def apply_config_override(
        self,
        model: "LLMModel",
        body: dict[str, Any],
        config: "LLMGenerationConfig | None" = None,
    ) -> dict[str, Any]:
        """åº”ç”¨é…ç½®è¦†ç›– - Gemini ä¸éœ€è¦é¢å¤–çš„é…ç½®è¦†ç›–"""
        return body

    def _get_gemini_endpoint(
        self, model: "LLMModel", config: "LLMGenerationConfig | None" = None
    ) -> str:
        """æ ¹æ®é…ç½®é€‰æ‹©Gemini APIç«¯ç‚¹"""
        if config:
            if getattr(config, "enable_code_execution", False):
                return f"/v1beta/models/{model.model_name}:generateContent"

            if getattr(config, "enable_grounding", False):
                return f"/v1beta/models/{model.model_name}:generateContent"

        return f"/v1beta/models/{model.model_name}:generateContent"

    def _convert_tool_choice_to_gemini(
        self, tool_choice_value: str | dict[str, Any]
    ) -> dict[str, Any]:
        """è½¬æ¢å·¥å…·é€‰æ‹©ç­–ç•¥ä¸ºGeminiæ ¼å¼"""
        if isinstance(tool_choice_value, str):
            mode_upper = tool_choice_value.upper()
            if mode_upper in ["AUTO", "NONE", "ANY"]:
                return {"functionCallingConfig": {"mode": mode_upper}}
            else:
                logger.warning(
                    f"ä¸æ”¯æŒçš„ tool_choice å­—ç¬¦ä¸²å€¼: '{tool_choice_value}'ã€‚"
                    f"å›é€€åˆ° AUTOã€‚"
                )
                return {"functionCallingConfig": {"mode": "AUTO"}}

        elif isinstance(tool_choice_value, dict):
            if (
                tool_choice_value.get("type") == "function"
                and "function" in tool_choice_value
            ):
                func_name = tool_choice_value["function"].get("name")
                if func_name:
                    return {
                        "functionCallingConfig": {
                            "mode": "ANY",
                            "allowedFunctionNames": [func_name],
                        }
                    }
                else:
                    logger.warning(
                        f"tool_choice dict ä¸­çš„å‡½æ•°åæ— æ•ˆ: {tool_choice_value}ã€‚"
                        f"å›é€€åˆ° AUTOã€‚"
                    )
                    return {"functionCallingConfig": {"mode": "AUTO"}}

            elif "functionCallingConfig" in tool_choice_value:
                return {
                    "functionCallingConfig": tool_choice_value["functionCallingConfig"]
                }

            else:
                logger.warning(
                    f"ä¸æ”¯æŒçš„ tool_choice dict å€¼: {tool_choice_value}ã€‚å›é€€åˆ° AUTOã€‚"
                )
                return {"functionCallingConfig": {"mode": "AUTO"}}

        logger.warning(
            f"tool_choice çš„ç±»å‹æ— æ•ˆ: {type(tool_choice_value)}ã€‚å›é€€åˆ° AUTOã€‚"
        )
        return {"functionCallingConfig": {"mode": "AUTO"}}

    def _build_gemini_generation_config(
        self, model: "LLMModel", config: "LLMGenerationConfig | None" = None
    ) -> dict[str, Any]:
        """æ„å»ºGeminiç”Ÿæˆé…ç½®"""
        effective_config = config if config is not None else model._generation_config

        if not effective_config:
            return {}

        generation_config = effective_config.to_api_params(
            api_type="gemini", model_name=model.model_name
        )

        if generation_config:
            param_keys = list(generation_config.keys())
            logger.debug(
                f"æ„å»ºGeminiç”Ÿæˆé…ç½®å®Œæˆï¼ŒåŒ…å« {len(generation_config)} ä¸ªå‚æ•°: "
                f"{param_keys}"
            )

        return generation_config

    def _build_safety_settings(
        self, config: "LLMGenerationConfig | None" = None
    ) -> list[dict[str, Any]] | None:
        """æ„å»ºå®‰å…¨è®¾ç½®"""
        if not config:
            return None

        safety_settings = []

        safety_categories = [
            "HARM_CATEGORY_HARASSMENT",
            "HARM_CATEGORY_HATE_SPEECH",
            "HARM_CATEGORY_SEXUALLY_EXPLICIT",
            "HARM_CATEGORY_DANGEROUS_CONTENT",
        ]

        custom_safety_settings = getattr(config, "safety_settings", None)
        if custom_safety_settings:
            for category, threshold in custom_safety_settings.items():
                safety_settings.append({"category": category, "threshold": threshold})
        else:
            from ..config.providers import get_gemini_safety_threshold

            threshold = get_gemini_safety_threshold()
            for category in safety_categories:
                safety_settings.append({"category": category, "threshold": threshold})

        return safety_settings if safety_settings else None

    def parse_response(
        self,
        model: "LLMModel",
        response_json: dict[str, Any],
        is_advanced: bool = False,
    ) -> ResponseData:
        """è§£æAPIå“åº”"""
        return self._parse_response(model, response_json, is_advanced)

    def _parse_response(
        self,
        model: "LLMModel",
        response_json: dict[str, Any],
        is_advanced: bool = False,
    ) -> ResponseData:
        """è§£æ Gemini API å“åº”"""
        _ = is_advanced
        self.validate_response(response_json)

        try:
            candidates = response_json.get("candidates", [])
            if not candidates:
                logger.debug("Geminiå“åº”ä¸­æ²¡æœ‰candidatesã€‚")
                return ResponseData(text="", raw_response=response_json)

            candidate = candidates[0]

            if candidate.get("finishReason") in [
                "RECITATION",
                "OTHER",
            ] and not candidate.get("content"):
                logger.warning(
                    f"Gemini candidate finished with reason "
                    f"'{candidate.get('finishReason')}' and no content."
                )
                return ResponseData(
                    text="",
                    raw_response=response_json,
                    usage_info=response_json.get("usageMetadata"),
                )

            content_data = candidate.get("content", {})
            parts = content_data.get("parts", [])

            text_content = ""
            parsed_tool_calls: list["LLMToolCall"] | None = None
            thought_summary_parts = []
            answer_parts = []

            for part in parts:
                if "text" in part:
                    answer_parts.append(part["text"])
                elif "thought" in part:
                    thought_summary_parts.append(part["thought"])
                elif "thoughtSummary" in part:
                    thought_summary_parts.append(part["thoughtSummary"])
                elif "functionCall" in part:
                    if parsed_tool_calls is None:
                        parsed_tool_calls = []
                    fc_data = part["functionCall"]
                    try:
                        import json

                        from ..types.models import LLMToolCall, LLMToolFunction

                        call_id = f"call_{model.provider_name}_{len(parsed_tool_calls)}"
                        parsed_tool_calls.append(
                            LLMToolCall(
                                id=call_id,
                                function=LLMToolFunction(
                                    name=fc_data["name"],
                                    arguments=json.dumps(fc_data["args"]),
                                ),
                            )
                        )
                    except KeyError as e:
                        logger.warning(
                            f"è§£æGemini functionCallæ—¶ç¼ºå°‘é”®: {fc_data}, é”™è¯¯: {e}"
                        )
                    except Exception as e:
                        logger.warning(
                            f"è§£æGemini functionCallæ—¶å‡ºé”™: {fc_data}, é”™è¯¯: {e}"
                        )
                elif "codeExecutionResult" in part:
                    result = part["codeExecutionResult"]
                    if result.get("outcome") == "OK":
                        output = result.get("output", "")
                        answer_parts.append(f"\n[ä»£ç æ‰§è¡Œç»“æœ]:\n```\n{output}\n```\n")
                    else:
                        answer_parts.append(
                            f"\n[ä»£ç æ‰§è¡Œå¤±è´¥]: {result.get('outcome', 'UNKNOWN')}\n"
                        )

            if thought_summary_parts:
                full_thought_summary = "\n".join(thought_summary_parts).strip()
                full_answer = "".join(answer_parts).strip()

                formatted_parts = []
                if full_thought_summary:
                    formatted_parts.append(f"ğŸ¤” **æ€è€ƒè¿‡ç¨‹**\n\n{full_thought_summary}")
                if full_answer:
                    separator = "\n\n---\n\n" if full_thought_summary else ""
                    formatted_parts.append(f"{separator}âœ… **å›ç­”**\n\n{full_answer}")

                text_content = "".join(formatted_parts)
            else:
                text_content = "".join(answer_parts)

            usage_info = response_json.get("usageMetadata")

            grounding_metadata_obj = None
            if grounding_data := candidate.get("groundingMetadata"):
                try:
                    from ..types.models import LLMGroundingMetadata

                    grounding_metadata_obj = LLMGroundingMetadata(**grounding_data)
                except Exception as e:
                    logger.warning(f"æ— æ³•è§£æGroundingå…ƒæ•°æ®: {grounding_data}, {e}")

            return ResponseData(
                text=text_content,
                tool_calls=parsed_tool_calls,
                usage_info=usage_info,
                raw_response=response_json,
                grounding_metadata=grounding_metadata_obj,
            )

        except Exception as e:
            logger.error(f"è§£æ Gemini å“åº”å¤±è´¥: {e}", e=e)
            raise LLMException(
                f"è§£æAPIå“åº”å¤±è´¥: {e}",
                code=LLMErrorCode.RESPONSE_PARSE_ERROR,
                cause=e,
            )

    def prepare_embedding_request(
        self,
        model: "LLMModel",
        api_key: str,
        texts: list[str],
        task_type: "EmbeddingTaskType | str",
        **kwargs: Any,
    ) -> RequestData:
        """å‡†å¤‡æ–‡æœ¬åµŒå…¥è¯·æ±‚"""
        api_model_name = model.model_name
        if not api_model_name.startswith("models/"):
            api_model_name = f"models/{api_model_name}"

        url = self.get_api_url(model, f"/{api_model_name}:batchEmbedContents")
        headers = self.get_base_headers(api_key)

        requests_payload = []
        for text_content in texts:
            request_item: dict[str, Any] = {
                "content": {"parts": [{"text": text_content}]},
            }

            from ..types.enums import EmbeddingTaskType

            if task_type and task_type != EmbeddingTaskType.RETRIEVAL_DOCUMENT:
                request_item["task_type"] = str(task_type).upper()
            if title := kwargs.get("title"):
                request_item["title"] = title
            if output_dimensionality := kwargs.get("output_dimensionality"):
                request_item["output_dimensionality"] = output_dimensionality

            requests_payload.append(request_item)

        body = {"requests": requests_payload}
        return RequestData(url=url, headers=headers, body=body)

    def parse_embedding_response(
        self, response_json: dict[str, Any]
    ) -> list[list[float]]:
        """è§£ææ–‡æœ¬åµŒå…¥å“åº”"""
        try:
            embeddings_data = response_json["embeddings"]
            return [item["values"] for item in embeddings_data]
        except KeyError as e:
            logger.error(f"è§£æGeminiåµŒå…¥å“åº”æ—¶ç¼ºå°‘é”®: {e}. å“åº”: {response_json}")
            raise LLMException(
                "GeminiåµŒå…¥å“åº”æ ¼å¼é”™è¯¯",
                code=LLMErrorCode.RESPONSE_PARSE_ERROR,
                details={"error": str(e)},
            )
        except Exception as e:
            logger.error(
                f"è§£æGeminiåµŒå…¥å“åº”æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}. å“åº”: {response_json}"
            )
            raise LLMException(
                f"è§£æGeminiåµŒå…¥å“åº”å¤±è´¥: {e}",
                code=LLMErrorCode.RESPONSE_PARSE_ERROR,
                cause=e,
            )

    def validate_embedding_response(self, response_json: dict[str, Any]) -> None:
        """éªŒè¯åµŒå…¥å“åº”"""
        super().validate_embedding_response(response_json)
        if "embeddings" not in response_json or not isinstance(
            response_json["embeddings"], list
        ):
            raise LLMException(
                "GeminiåµŒå…¥å“åº”ç¼ºå°‘'embeddings'å­—æ®µæˆ–æ ¼å¼ä¸æ­£ç¡®",
                code=LLMErrorCode.RESPONSE_PARSE_ERROR,
                details=response_json,
            )
        for item in response_json["embeddings"]:
            if "values" not in item:
                raise LLMException(
                    "GeminiåµŒå…¥å“åº”çš„æ¡ç›®ä¸­ç¼ºå°‘'values'å­—æ®µ",
                    code=LLMErrorCode.RESPONSE_PARSE_ERROR,
                    details=response_json,
                )
