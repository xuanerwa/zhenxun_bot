# Zhenxun LLM æœåŠ¡æ¨¡å—

## ğŸ“‘ ç›®å½•

- [ğŸ“– æ¦‚è¿°](#-æ¦‚è¿°)
- [ğŸŒŸ ä¸»è¦ç‰¹æ€§](#-ä¸»è¦ç‰¹æ€§)
- [ğŸš€ å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)
- [ğŸ“š API å‚è€ƒ](#-api-å‚è€ƒ)
- [âš™ï¸ é…ç½®](#ï¸-é…ç½®)
- [ğŸ”§ é«˜çº§åŠŸèƒ½](#-é«˜çº§åŠŸèƒ½)
- [ğŸ—ï¸ æ¶æ„è®¾è®¡](#ï¸-æ¶æ„è®¾è®¡)
- [ğŸ”Œ æ”¯æŒçš„æä¾›å•†](#-æ”¯æŒçš„æä¾›å•†)
- [ğŸ¯ ä½¿ç”¨åœºæ™¯](#-ä½¿ç”¨åœºæ™¯)
- [ğŸ“Š æ€§èƒ½ä¼˜åŒ–](#-æ€§èƒ½ä¼˜åŒ–)
- [ğŸ› ï¸ æ•…éšœæ’é™¤](#ï¸-æ•…éšœæ’é™¤)
- [â“ å¸¸è§é—®é¢˜](#-å¸¸è§é—®é¢˜)
- [ğŸ“ ç¤ºä¾‹é¡¹ç›®](#-ç¤ºä¾‹é¡¹ç›®)
- [ğŸ¤ è´¡çŒ®](#-è´¡çŒ®)
- [ğŸ“„ è®¸å¯è¯](#-è®¸å¯è¯)

## ğŸ“– æ¦‚è¿°

Zhenxun LLM æœåŠ¡æ¨¡å—æ˜¯ä¸€ä¸ªç°ä»£åŒ–çš„AIæœåŠ¡æ¡†æ¶ï¼Œæä¾›ç»Ÿä¸€çš„æ¥å£æ¥è®¿é—®å¤šä¸ªå¤§è¯­è¨€æ¨¡å‹æä¾›å•†ã€‚è¯¥æ¨¡å—é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œæ”¯æŒå¼‚æ­¥æ“ä½œã€æ™ºèƒ½é‡è¯•ã€Keyè½®è¯¢å’Œè´Ÿè½½å‡è¡¡ç­‰é«˜çº§åŠŸèƒ½ã€‚

### ğŸŒŸ ä¸»è¦ç‰¹æ€§

- **å¤šæä¾›å•†æ”¯æŒ**: OpenAIã€Geminiã€æ™ºè°±AIã€DeepSeekç­‰
- **ç»Ÿä¸€æ¥å£**: ç®€æ´ä¸€è‡´çš„APIè®¾è®¡
- **æ™ºèƒ½Keyè½®è¯¢**: è‡ªåŠ¨è´Ÿè½½å‡è¡¡å’Œæ•…éšœè½¬ç§»
- **å¼‚æ­¥é«˜æ€§èƒ½**: åŸºäºasyncioçš„å¹¶å‘å¤„ç†
- **æ¨¡å‹ç¼“å­˜**: æ™ºèƒ½ç¼“å­˜æœºåˆ¶æå‡æ€§èƒ½
- **å·¥å…·è°ƒç”¨**: æ”¯æŒFunction Calling
- **åµŒå…¥å‘é‡**: æ–‡æœ¬å‘é‡åŒ–æ”¯æŒ
- **é”™è¯¯å¤„ç†**: å®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œé‡è¯•æœºåˆ¶
- **å¤šæ¨¡æ€æ”¯æŒ**: æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘å¤„ç†
- **ä»£ç æ‰§è¡Œ**: Geminiä»£ç æ‰§è¡ŒåŠŸèƒ½
- **æœç´¢å¢å¼º**: Googleæœç´¢é›†æˆ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºæœ¬ä½¿ç”¨

```python
from zhenxun.services.llm import chat, code, search, analyze

# ç®€å•èŠå¤©
response = await chat("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±")
print(response)

# ä»£ç æ‰§è¡Œ
result = await code("è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‰10é¡¹")
print(result["text"])
print(result["code_executions"])

# æœç´¢åŠŸèƒ½
search_result = await search("Pythonå¼‚æ­¥ç¼–ç¨‹æœ€ä½³å®è·µ")
print(search_result["text"])

# å¤šæ¨¡æ€åˆ†æ
from nonebot_plugin_alconna.uniseg import UniMessage, Image, Text
message = UniMessage([
    Text("åˆ†æè¿™å¼ å›¾ç‰‡"),
    Image(path="image.jpg")
])
analysis = await analyze(message, model="Gemini/gemini-2.0-flash")
print(analysis)
```

### ä½¿ç”¨AIç±»

```python
from zhenxun.services.llm import AI, AIConfig, CommonOverrides

# åˆ›å»ºAIå®ä¾‹
ai = AI(AIConfig(model="OpenAI/gpt-4"))

# èŠå¤©å¯¹è¯
response = await ai.chat("è§£é‡Šé‡å­è®¡ç®—çš„åŸºæœ¬åŸç†")

# å¤šæ¨¡æ€åˆ†æ
from nonebot_plugin_alconna.uniseg import UniMessage, Image, Text

multimodal_msg = UniMessage([
    Text("è¿™å¼ å›¾ç‰‡æ˜¾ç¤ºäº†ä»€ä¹ˆï¼Ÿ"),
    Image(path="image.jpg")
])
result = await ai.analyze(multimodal_msg)

# ä¾¿æ·çš„å¤šæ¨¡æ€å‡½æ•°
result = await analyze_with_images(
    "åˆ†æè¿™å¼ å›¾ç‰‡",
    images="image.jpg",
    model="Gemini/gemini-2.0-flash"
)
```

## ğŸ“š API å‚è€ƒ

### å¿«é€Ÿå‡½æ•°

#### `chat(message, *, model=None, **kwargs) -> str`
ç®€å•èŠå¤©å¯¹è¯

**å‚æ•°:**
- `message`: æ¶ˆæ¯å†…å®¹ï¼ˆå­—ç¬¦ä¸²ã€LLMMessageæˆ–å†…å®¹éƒ¨åˆ†åˆ—è¡¨ï¼‰
- `model`: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
- `**kwargs`: é¢å¤–é…ç½®å‚æ•°

#### `code(prompt, *, model=None, timeout=None, **kwargs) -> dict`
ä»£ç æ‰§è¡ŒåŠŸèƒ½

**è¿”å›:**
```python
{
    "text": "æ‰§è¡Œç»“æœè¯´æ˜",
    "code_executions": [{"code": "...", "output": "..."}],
    "success": True
}
```

#### `search(query, *, model=None, instruction="", **kwargs) -> dict`
æœç´¢å¢å¼ºç”Ÿæˆ

**è¿”å›:**
```python
{
    "text": "æœç´¢ç»“æœå’Œåˆ†æ",
    "grounding_metadata": {...},
    "success": True
}
```

#### `analyze(message, *, instruction="", model=None, tools=None, tool_config=None, **kwargs) -> str | LLMResponse`
é«˜çº§åˆ†æåŠŸèƒ½ï¼Œæ”¯æŒå¤šæ¨¡æ€è¾“å…¥å’Œå·¥å…·è°ƒç”¨

#### `analyze_with_images(text, images, *, instruction="", model=None, **kwargs) -> str`
å›¾ç‰‡åˆ†æä¾¿æ·å‡½æ•°

#### `analyze_multimodal(text=None, images=None, videos=None, audios=None, *, instruction="", model=None, **kwargs) -> str`
å¤šæ¨¡æ€åˆ†æä¾¿æ·å‡½æ•°

#### `embed(texts, *, model=None, task_type="RETRIEVAL_DOCUMENT", **kwargs) -> list[list[float]]`
æ–‡æœ¬åµŒå…¥å‘é‡

### AIç±»æ–¹æ³•

#### `AI.chat(message, *, model=None, **kwargs) -> str`
èŠå¤©å¯¹è¯æ–¹æ³•ï¼Œæ”¯æŒç®€å•å¤šæ¨¡æ€è¾“å…¥

#### `AI.analyze(message, *, instruction="", model=None, tools=None, tool_config=None, **kwargs) -> str | LLMResponse`
é«˜çº§åˆ†ææ–¹æ³•ï¼Œæ¥æ”¶UniMessageè¿›è¡Œå¤šæ¨¡æ€åˆ†æå’Œå·¥å…·è°ƒç”¨

### æ¨¡å‹ç®¡ç†

```python
from zhenxun.services.llm import (
    get_model_instance,
    list_available_models,
    set_global_default_model_name,
    clear_model_cache
)

# è·å–æ¨¡å‹å®ä¾‹
model = await get_model_instance("OpenAI/gpt-4o")

# åˆ—å‡ºå¯ç”¨æ¨¡å‹
models = list_available_models()

# è®¾ç½®é»˜è®¤æ¨¡å‹
set_global_default_model_name("Gemini/gemini-2.0-flash")

# æ¸…ç†ç¼“å­˜
clear_model_cache()
```

## âš™ï¸ é…ç½®

### é¢„è®¾é…ç½®

```python
from zhenxun.services.llm import CommonOverrides

# åˆ›æ„æ¨¡å¼
creative_config = CommonOverrides.creative()

# ç²¾ç¡®æ¨¡å¼
precise_config = CommonOverrides.precise()

# Geminiç‰¹æ®ŠåŠŸèƒ½
json_config = CommonOverrides.gemini_json()
thinking_config = CommonOverrides.gemini_thinking()
code_exec_config = CommonOverrides.gemini_code_execution()
grounding_config = CommonOverrides.gemini_grounding()
```

### è‡ªå®šä¹‰é…ç½®

```python
from zhenxun.services.llm import LLMGenerationConfig

config = LLMGenerationConfig(
    temperature=0.7,
    max_tokens=2048,
    top_p=0.9,
    frequency_penalty=0.1,
    presence_penalty=0.1,
    stop=["END", "STOP"],
    response_mime_type="application/json",
    enable_code_execution=True,
    enable_grounding=True
)

response = await chat("ä½ çš„é—®é¢˜", override_config=config)
```

## ğŸ”§ é«˜çº§åŠŸèƒ½

### å·¥å…·è°ƒç”¨ (Function Calling)

```python
from zhenxun.services.llm import LLMTool, get_model_instance

# å®šä¹‰å·¥å…·
tools = [
    LLMTool(
        name="get_weather",
        description="è·å–å¤©æ°”ä¿¡æ¯",
        parameters={
            "type": "object",
            "properties": {
                "city": {"type": "string", "description": "åŸå¸‚åç§°"}
            },
            "required": ["city"]
        }
    )
]

# å·¥å…·æ‰§è¡Œå™¨
async def tool_executor(tool_name: str, args: dict) -> str:
    if tool_name == "get_weather":
        return f"{args['city']}ä»Šå¤©æ™´å¤©ï¼Œ25Â°C"
    return "æœªçŸ¥å·¥å…·"

# ä½¿ç”¨å·¥å…·
model = await get_model_instance("OpenAI/gpt-4")
response = await model.generate_response(
    messages=[{"role": "user", "content": "åŒ—äº¬å¤©æ°”å¦‚ä½•ï¼Ÿ"}],
    tools=tools,
    tool_executor=tool_executor
)
```

### å¤šæ¨¡æ€å¤„ç†

```python
from zhenxun.services.llm import create_multimodal_message, analyze_multimodal, analyze_with_images

# æ–¹æ³•1ï¼šä½¿ç”¨ä¾¿æ·å‡½æ•°
result = await analyze_multimodal(
    text="åˆ†æè¿™äº›åª’ä½“æ–‡ä»¶",
    images="image.jpg",
    audios="audio.mp3",
    model="Gemini/gemini-2.0-flash"
)

# æ–¹æ³•2ï¼šä½¿ç”¨create_multimodal_message
message = create_multimodal_message(
    text="åˆ†æè¿™å¼ å›¾ç‰‡å’ŒéŸ³é¢‘",
    images="image.jpg",
    audios="audio.mp3"
)
result = await analyze(message)

# æ–¹æ³•3ï¼šå›¾ç‰‡åˆ†æä¸“ç”¨å‡½æ•°
result = await analyze_with_images(
    "è¿™å¼ å›¾ç‰‡æ˜¾ç¤ºäº†ä»€ä¹ˆï¼Ÿ",
    images=["image1.jpg", "image2.jpg"]
)
```

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è§é”™è¯¯

1. **é…ç½®é”™è¯¯**: æ£€æŸ¥APIå¯†é’¥å’Œæ¨¡å‹é…ç½®
2. **ç½‘ç»œé—®é¢˜**: æ£€æŸ¥ä»£ç†è®¾ç½®å’Œç½‘ç»œè¿æ¥
3. **æ¨¡å‹ä¸å¯ç”¨**: ä½¿ç”¨ `list_available_models()` æ£€æŸ¥å¯ç”¨æ¨¡å‹
4. **è¶…æ—¶é”™è¯¯**: è°ƒæ•´timeoutå‚æ•°æˆ–ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹

### è°ƒè¯•æŠ€å·§

```python
from zhenxun.services.llm import get_cache_stats
from zhenxun.services.log import logger

# æŸ¥çœ‹ç¼“å­˜çŠ¶æ€
stats = get_cache_stats()
print(f"ç¼“å­˜å‘½ä¸­ç‡: {stats['hit_rate']}")

# å¯ç”¨è¯¦ç»†æ—¥å¿—
logger.setLevel("DEBUG")
```

## â“ å¸¸è§é—®é¢˜


### Q: å¦‚ä½•å¤„ç†å¤šæ¨¡æ€è¾“å…¥ï¼Ÿ

**A:** æœ‰å¤šç§æ–¹å¼å¤„ç†å¤šæ¨¡æ€è¾“å…¥ï¼š
```python
# æ–¹æ³•1ï¼šä½¿ç”¨ä¾¿æ·å‡½æ•°
result = await analyze_with_images("åˆ†æè¿™å¼ å›¾ç‰‡", images="image.jpg")

# æ–¹æ³•2ï¼šä½¿ç”¨analyzeå‡½æ•°
from nonebot_plugin_alconna.uniseg import UniMessage, Image, Text
message = UniMessage([Text("åˆ†æè¿™å¼ å›¾ç‰‡"), Image(path="image.jpg")])
result = await analyze(message)

# æ–¹æ³•3ï¼šä½¿ç”¨create_multimodal_message
from zhenxun.services.llm import create_multimodal_message
message = create_multimodal_message(text="åˆ†æè¿™å¼ å›¾ç‰‡", images="image.jpg")
result = await analyze(message)
```

### Q: å¦‚ä½•è‡ªå®šä¹‰å·¥å…·è°ƒç”¨ï¼Ÿ

**A:** ä½¿ç”¨analyzeå‡½æ•°çš„toolså‚æ•°ï¼š
```python
# å®šä¹‰å·¥å…·
tools = [{
    "name": "calculator",
    "description": "è®¡ç®—æ•°å­¦è¡¨è¾¾å¼",
    "parameters": {
        "type": "object",
        "properties": {
            "expression": {"type": "string", "description": "æ•°å­¦è¡¨è¾¾å¼"}
        },
        "required": ["expression"]
    }
}]

# ä½¿ç”¨å·¥å…·
from nonebot_plugin_alconna.uniseg import UniMessage, Text
message = UniMessage([Text("è®¡ç®— 2+3*4")])
response = await analyze(message, tools=tools, tool_config={"mode": "auto"})

# å¦‚æœè¿”å›LLMResponseï¼Œè¯´æ˜æœ‰å·¥å…·è°ƒç”¨
if hasattr(response, 'tool_calls'):
    for tool_call in response.tool_calls:
        print(f"è°ƒç”¨å·¥å…·: {tool_call.function.name}")
        print(f"å‚æ•°: {tool_call.function.arguments}")
```


### Q: å¦‚ä½•ç¡®ä¿è¾“å‡ºæ ¼å¼ï¼Ÿ

**A:** ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºï¼š
```python
# JSONæ ¼å¼è¾“å‡º
config = CommonOverrides.gemini_json()

# è‡ªå®šä¹‰Schema
schema = {
    "type": "object",
    "properties": {
        "answer": {"type": "string"},
        "confidence": {"type": "number"}
    }
}
config = CommonOverrides.gemini_structured(schema)
```

## ğŸ“ ç¤ºä¾‹é¡¹ç›®

### å®Œæ•´ç¤ºä¾‹

#### 1. æ™ºèƒ½å®¢æœæœºå™¨äºº

```python
from zhenxun.services.llm import AI, CommonOverrides
from typing import Dict, List

class CustomerService:
    def __init__(self):
        self.ai = AI()
        self.sessions: Dict[str, List[dict]] = {}

    async def handle_query(self, user_id: str, query: str) -> str:
        # è·å–æˆ–åˆ›å»ºä¼šè¯å†å²
        if user_id not in self.sessions:
            self.sessions[user_id] = []

        history = self.sessions[user_id]

        # æ·»åŠ ç³»ç»Ÿæç¤º
        if not history:
            history.append({
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¢æœåŠ©æ‰‹ï¼Œè¯·å‹å¥½ã€å‡†ç¡®åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
            })

        # æ·»åŠ ç”¨æˆ·é—®é¢˜
        history.append({"role": "user", "content": query})

        # ç”Ÿæˆå›å¤
        response = await self.ai.chat(
            query,
            history=history[-20:],  # ä¿ç•™æœ€è¿‘20è½®å¯¹è¯
            override_config=CommonOverrides.balanced()
        )

        # ä¿å­˜å›å¤åˆ°å†å²
        history.append({"role": "assistant", "content": response})

        return response
```

#### 2. æ–‡æ¡£æ™ºèƒ½é—®ç­”

```python
from zhenxun.services.llm import embed, analyze
import numpy as np
from typing import List, Tuple

class DocumentQA:
    def __init__(self):
        self.documents: List[str] = []
        self.embeddings: List[List[float]] = []

    async def add_document(self, text: str):
        """æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
        self.documents.append(text)

        # ç”ŸæˆåµŒå…¥å‘é‡
        embedding = await embed([text])
        self.embeddings.extend(embedding)

    async def query(self, question: str, top_k: int = 3) -> str:
        """æŸ¥è¯¢æ–‡æ¡£å¹¶ç”Ÿæˆç­”æ¡ˆ"""
        if not self.documents:
            return "çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆæ·»åŠ æ–‡æ¡£ã€‚"

        # ç”Ÿæˆé—®é¢˜çš„åµŒå…¥å‘é‡
        question_embedding = await embed([question])

        # è®¡ç®—ç›¸ä¼¼åº¦å¹¶æ‰¾åˆ°æœ€ç›¸å…³çš„æ–‡æ¡£
        similarities = []
        for doc_embedding in self.embeddings:
            similarity = np.dot(question_embedding[0], doc_embedding)
            similarities.append(similarity)

        # è·å–æœ€ç›¸å…³çš„æ–‡æ¡£
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        relevant_docs = [self.documents[i] for i in top_indices]

        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join(relevant_docs)
        prompt = f"""
åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼š

æ–‡æ¡£å†…å®¹ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·åŸºäºæ–‡æ¡£å†…å®¹ç»™å‡ºå‡†ç¡®çš„ç­”æ¡ˆï¼Œå¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜ã€‚
"""

        result = await analyze(prompt)
        return result["text"]
```

#### 3. ä»£ç å®¡æŸ¥åŠ©æ‰‹

```python
from zhenxun.services.llm import code, analyze
import os

class CodeReviewer:
    async def review_file(self, file_path: str) -> dict:
        """å®¡æŸ¥ä»£ç æ–‡ä»¶"""
        if not os.path.exists(file_path):
            return {"error": "æ–‡ä»¶ä¸å­˜åœ¨"}

        with open(file_path, 'r', encoding='utf-8') as f:
            code_content = f.read()

        prompt = f"""
è¯·å®¡æŸ¥ä»¥ä¸‹ä»£ç ï¼Œæä¾›è¯¦ç»†çš„åé¦ˆï¼š

æ–‡ä»¶ï¼š{file_path}
ä»£ç ï¼š
```
{code_content}
```

è¯·ä»ä»¥ä¸‹æ–¹é¢è¿›è¡Œå®¡æŸ¥ï¼š
1. ä»£ç è´¨é‡å’Œå¯è¯»æ€§
2. æ½œåœ¨çš„bugå’Œå®‰å…¨é—®é¢˜
3. æ€§èƒ½ä¼˜åŒ–å»ºè®®
4. æœ€ä½³å®è·µå»ºè®®
5. ä»£ç é£æ ¼é—®é¢˜

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœã€‚
"""

        result = await analyze(
            prompt,
            model="DeepSeek/deepseek-coder",
            override_config=CommonOverrides.gemini_json()
        )

        return {
            "file": file_path,
            "review": result["text"],
            "success": True
        }

    async def suggest_improvements(self, code: str, language: str = "python") -> str:
        """å»ºè®®ä»£ç æ”¹è¿›"""
        prompt = f"""
è¯·æ”¹è¿›ä»¥ä¸‹{language}ä»£ç ï¼Œä½¿å…¶æ›´åŠ é«˜æ•ˆã€å¯è¯»å’Œç¬¦åˆæœ€ä½³å®è·µï¼š

åŸä»£ç ï¼š
```{language}
{code}
```

è¯·æä¾›æ”¹è¿›åçš„ä»£ç å’Œè¯´æ˜ã€‚
"""

        result = await code(prompt, model="DeepSeek/deepseek-coder")
        return result["text"]
```


## ğŸ—ï¸ æ¶æ„è®¾è®¡

### æ¨¡å—ç»“æ„

```
zhenxun/services/llm/
â”œâ”€â”€ __init__.py          # åŒ…å…¥å£ï¼Œå¯¼å…¥å’Œæš´éœ²å…¬å…±API
â”œâ”€â”€ api.py              # é«˜çº§APIæ¥å£ï¼ˆAIç±»ã€ä¾¿æ·å‡½æ•°ï¼‰
â”œâ”€â”€ core.py             # æ ¸å¿ƒåŸºç¡€è®¾æ–½ï¼ˆHTTPå®¢æˆ·ç«¯ã€é‡è¯•é€»è¾‘ã€KeyStoreï¼‰
â”œâ”€â”€ service.py          # LLMæ¨¡å‹å®ç°ç±»
â”œâ”€â”€ utils.py            # å·¥å…·å’Œè½¬æ¢å‡½æ•°
â”œâ”€â”€ manager.py          # æ¨¡å‹ç®¡ç†å’Œç¼“å­˜
â”œâ”€â”€ adapters/           # é€‚é…å™¨æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py    # é€‚é…å™¨åŒ…å…¥å£
â”‚   â”œâ”€â”€ base.py        # åŸºç¡€é€‚é…å™¨
â”‚   â”œâ”€â”€ factory.py     # é€‚é…å™¨å·¥å‚
â”‚   â”œâ”€â”€ openai.py      # OpenAIé€‚é…å™¨
â”‚   â”œâ”€â”€ gemini.py      # Geminié€‚é…å™¨
â”‚   â””â”€â”€ zhipu.py       # æ™ºè°±AIé€‚é…å™¨
â”œâ”€â”€ config/            # é…ç½®æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py    # é…ç½®åŒ…å…¥å£
â”‚   â”œâ”€â”€ generation.py  # ç”Ÿæˆé…ç½®
â”‚   â”œâ”€â”€ presets.py     # é¢„è®¾é…ç½®
â”‚   â””â”€â”€ providers.py   # æä¾›å•†é…ç½®
â””â”€â”€ types/             # ç±»å‹å®šä¹‰
    â”œâ”€â”€ __init__.py    # ç±»å‹åŒ…å…¥å£
    â”œâ”€â”€ content.py     # å†…å®¹ç±»å‹
    â”œâ”€â”€ enums.py       # æšä¸¾å®šä¹‰
    â”œâ”€â”€ exceptions.py  # å¼‚å¸¸å®šä¹‰
    â””â”€â”€ models.py      # æ•°æ®æ¨¡å‹
```

### æ¨¡å—èŒè´£

- **`__init__.py`**: çº¯ç²¹çš„åŒ…å…¥å£ï¼Œåªè´Ÿè´£å¯¼å…¥å’Œæš´éœ²å…¬å…±API
- **`api.py`**: é«˜çº§APIæ¥å£ï¼ŒåŒ…å«AIç±»å’Œæ‰€æœ‰ä¾¿æ·å‡½æ•°
- **`core.py`**: æ ¸å¿ƒåŸºç¡€è®¾æ–½ï¼ŒåŒ…å«HTTPå®¢æˆ·ç«¯ç®¡ç†ã€é‡è¯•é€»è¾‘å’ŒKeyStore
- **`service.py`**: LLMæ¨¡å‹å®ç°ç±»ï¼Œä¸“æ³¨äºæ¨¡å‹é€»è¾‘
- **`utils.py`**: å·¥å…·å’Œè½¬æ¢å‡½æ•°ï¼Œå¦‚å¤šæ¨¡æ€æ¶ˆæ¯å¤„ç†
- **`manager.py`**: æ¨¡å‹ç®¡ç†å’Œç¼“å­˜æœºåˆ¶
- **`adapters/`**: å„å¤§æä¾›å•†çš„é€‚é…å™¨æ¨¡å—ï¼Œè´Ÿè´£ä¸ä¸åŒAPIçš„äº¤äº’
  - `base.py`: å®šä¹‰é€‚é…å™¨çš„åŸºç¡€æ¥å£
  - `factory.py`: é€‚é…å™¨å·¥å‚ï¼Œç”¨äºåŠ¨æ€åŠ è½½å’Œå®ä¾‹åŒ–é€‚é…å™¨
  - `openai.py`: OpenAI APIé€‚é…å™¨
  - `gemini.py`: Google Gemini APIé€‚é…å™¨
  - `zhipu.py`: æ™ºè°±AI APIé€‚é…å™¨
- **`config/`**: é…ç½®ç®¡ç†æ¨¡å—
  - `generation.py`: ç”Ÿæˆé…ç½®å’Œé¢„è®¾
  - `presets.py`: é¢„è®¾é…ç½®
  - `providers.py`: æä¾›å•†é…ç½®
- **`types/`**: ç±»å‹å®šä¹‰æ¨¡å—
  - `content.py`: å†…å®¹ç±»å‹å®šä¹‰
  - `enums.py`: æšä¸¾å®šä¹‰
  - `exceptions.py`: å¼‚å¸¸å®šä¹‰
  - `models.py`: æ•°æ®æ¨¡å‹å®šä¹‰

## ğŸ”Œ æ”¯æŒçš„æä¾›å•†

### OpenAI å…¼å®¹

- **OpenAI**: GPT-4o, GPT-3.5-turboç­‰
- **DeepSeek**: deepseek-chat, deepseek-reasonerç­‰
- **å…¶ä»–OpenAIå…¼å®¹API**: æ”¯æŒè‡ªå®šä¹‰ç«¯ç‚¹

```python
# OpenAI
await chat("Hello", model="OpenAI/gpt-4o")

# DeepSeek
await chat("å†™ä»£ç ", model="DeepSeek/deepseek-reasoner")
```

### Google Gemini

- **Gemini Pro**: gemini-2.5-flash-preview-05-20 gemini-2.0-flashç­‰
- **ç‰¹æ®ŠåŠŸèƒ½**: ä»£ç æ‰§è¡Œã€æœç´¢å¢å¼ºã€æ€è€ƒæ¨¡å¼

```python
# åŸºç¡€ä½¿ç”¨
await chat("ä½ å¥½", model="Gemini/gemini-2.0-flash")

# ä»£ç æ‰§è¡Œ
await code("è®¡ç®—è´¨æ•°", model="Gemini/gemini-2.0-flash")

# æœç´¢å¢å¼º
await search("æœ€æ–°AIå‘å±•", model="Gemini/gemini-2.5-flash-preview-05-20")
```

### æ™ºè°±AI

- **GLMç³»åˆ—**: glm-4, glm-4vç­‰
- **æ”¯æŒåŠŸèƒ½**: æ–‡æœ¬ç”Ÿæˆã€å¤šæ¨¡æ€ç†è§£

```python
await chat("ä»‹ç»åŒ—äº¬", model="Zhipu/glm-4")
```

## ğŸ¯ ä½¿ç”¨åœºæ™¯

### 1. èŠå¤©æœºå™¨äºº

```python
from zhenxun.services.llm import AI, CommonOverrides

class ChatBot:
    def __init__(self):
        self.ai = AI()
        self.history = []

    async def chat(self, user_input: str) -> str:
        # æ·»åŠ å†å²è®°å½•
        self.history.append({"role": "user", "content": user_input})

        # ç”Ÿæˆå›å¤
        response = await self.ai.chat(
            user_input,
            history=self.history[-10:],  # ä¿ç•™æœ€è¿‘10è½®å¯¹è¯
            override_config=CommonOverrides.balanced()
        )

        self.history.append({"role": "assistant", "content": response})
        return response
```

### 2. ä»£ç åŠ©æ‰‹

```python
async def code_assistant(task: str) -> dict:
    """ä»£ç ç”Ÿæˆå’Œæ‰§è¡ŒåŠ©æ‰‹"""
    result = await code(
        f"è¯·å¸®æˆ‘{task}ï¼Œå¹¶æ‰§è¡Œä»£ç éªŒè¯ç»“æœ",
        model="Gemini/gemini-2.0-flash",
        timeout=60
    )

    return {
        "explanation": result["text"],
        "code_blocks": result["code_executions"],
        "success": result["success"]
    }

# ä½¿ç”¨ç¤ºä¾‹
result = await code_assistant("å®ç°å¿«é€Ÿæ’åºç®—æ³•")
```

### 3. æ–‡æ¡£åˆ†æ

```python
from zhenxun.services.llm import analyze_with_images

async def analyze_document(image_path: str, question: str) -> str:
    """åˆ†ææ–‡æ¡£å›¾ç‰‡å¹¶å›ç­”é—®é¢˜"""
    result = await analyze_with_images(
        f"è¯·åˆ†æè¿™ä¸ªæ–‡æ¡£å¹¶å›ç­”ï¼š{question}",
        images=image_path,
        model="Gemini/gemini-2.0-flash"
    )
    return result
```

### 4. æ™ºèƒ½æœç´¢

```python
async def smart_search(query: str) -> dict:
    """æ™ºèƒ½æœç´¢å’Œæ€»ç»“"""
    result = await search(
        query,
        model="Gemini/gemini-2.0-flash",
        instruction="è¯·æä¾›å‡†ç¡®ã€æœ€æ–°çš„ä¿¡æ¯ï¼Œå¹¶æ³¨æ˜ä¿¡æ¯æ¥æº"
    )

    return {
        "summary": result["text"],
        "sources": result.get("grounding_metadata", {}),
        "confidence": result.get("confidence_score", 0.0)
    }
```

## ğŸ”§ é…ç½®ç®¡ç†


### åŠ¨æ€é…ç½®

```python
from zhenxun.services.llm import set_global_default_model_name

# è¿è¡Œæ—¶æ›´æ”¹é»˜è®¤æ¨¡å‹
set_global_default_model_name("OpenAI/gpt-4")

# æ£€æŸ¥å¯ç”¨æ¨¡å‹
models = list_available_models()
for model in models:
    print(f"{model.provider}/{model.name} - {model.description}")
```

