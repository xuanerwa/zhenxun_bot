import asyncio
from collections.abc import Iterable
import contextlib
import time
from typing import Any, ClassVar
from typing_extensions import Self
from urllib.parse import urlparse

from nonebot import get_driver
from nonebot.utils import is_coroutine_callable
from tortoise import Tortoise
from tortoise.backends.base.client import BaseDBAsyncClient
from tortoise.connection import connections
from tortoise.exceptions import IntegrityError, MultipleObjectsReturned
from tortoise.models import Model as TortoiseModel
from tortoise.transactions import in_transaction

from zhenxun.configs.config import BotConfig
from zhenxun.services.cache import CacheRoot
from zhenxun.services.log import logger
from zhenxun.utils.enum import DbLockType
from zhenxun.utils.exception import HookPriorityException
from zhenxun.utils.manager.priority_manager import PriorityLifecycle

driver = get_driver()

SCRIPT_METHOD = []
MODELS: list[str] = []

# æ•°æ®åº“æ“ä½œè¶…æ—¶è®¾ç½®ï¼ˆç§’ï¼‰
DB_TIMEOUT_SECONDS = 3.0

# æ€§èƒ½ç›‘æ§é˜ˆå€¼ï¼ˆç§’ï¼‰
SLOW_QUERY_THRESHOLD = 0.5

LOG_COMMAND = "DbContext"


async def with_db_timeout(
    coro, timeout: float = DB_TIMEOUT_SECONDS, operation: str | None = None
):
    """å¸¦è¶…æ—¶æ§åˆ¶çš„æ•°æ®åº“æ“ä½œ"""
    start_time = time.time()
    try:
        result = await asyncio.wait_for(coro, timeout=timeout)
        elapsed = time.time() - start_time
        if elapsed > SLOW_QUERY_THRESHOLD and operation:
            logger.warning(f"æ…¢æŸ¥è¯¢: {operation} è€—æ—¶ {elapsed:.3f}s", LOG_COMMAND)
        return result
    except asyncio.TimeoutError:
        if operation:
            logger.error(f"æ•°æ®åº“æ“ä½œè¶…æ—¶: {operation} (>{timeout}s)", LOG_COMMAND)
        raise


class Model(TortoiseModel):
    """
    å¢å¼ºçš„ORMåŸºç±»ï¼Œè§£å†³é”åµŒå¥—é—®é¢˜
    """

    sem_data: ClassVar[dict[str, dict[str, asyncio.Semaphore]]] = {}
    _current_locks: ClassVar[dict[int, DbLockType]] = {}  # è·Ÿè¸ªå½“å‰åç¨‹æŒæœ‰çš„é”

    def __init_subclass__(cls, **kwargs):
        super().__init_subclass__(**kwargs)
        if cls.__module__ not in MODELS:
            MODELS.append(cls.__module__)

        if func := getattr(cls, "_run_script", None):
            SCRIPT_METHOD.append((cls.__module__, func))

    @classmethod
    def get_cache_type(cls) -> str | None:
        """è·å–ç¼“å­˜ç±»å‹"""
        return getattr(cls, "cache_type", None)

    @classmethod
    def get_cache_key_field(cls) -> str | tuple[str]:
        """è·å–ç¼“å­˜é”®å­—æ®µ"""
        return getattr(cls, "cache_key_field", "id")

    @classmethod
    def get_cache_key(cls, instance) -> str | None:
        """è·å–ç¼“å­˜é”®

        å‚æ•°:
            instance: æ¨¡å‹å®ä¾‹

        è¿”å›:
            str | None: ç¼“å­˜é”®ï¼Œå¦‚æœæ— æ³•è·å–åˆ™è¿”å›None
        """
        from zhenxun.services.cache.config import COMPOSITE_KEY_SEPARATOR

        key_field = cls.get_cache_key_field()

        if isinstance(key_field, tuple):
            # å¤šå­—æ®µä¸»é”®
            key_parts = []
            for field in key_field:
                if hasattr(instance, field):
                    value = getattr(instance, field, None)
                    key_parts.append(value if value is not None else "")
                else:
                    # å¦‚æœç¼ºå°‘ä»»ä½•å¿…è¦çš„å­—æ®µï¼Œè¿”å›None
                    key_parts.append("")

            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆå‚æ•°ï¼Œè¿”å›None
            return COMPOSITE_KEY_SEPARATOR.join(key_parts) if key_parts else None
        elif hasattr(instance, key_field):
            value = getattr(instance, key_field, None)
            return str(value) if value is not None else None

        return None

    @classmethod
    def get_semaphore(cls, lock_type: DbLockType):
        enable_lock = getattr(cls, "enable_lock", None)
        if not enable_lock or lock_type not in enable_lock:
            return None

        if cls.__name__ not in cls.sem_data:
            cls.sem_data[cls.__name__] = {}
        if lock_type not in cls.sem_data[cls.__name__]:
            cls.sem_data[cls.__name__][lock_type] = asyncio.Semaphore(1)
        return cls.sem_data[cls.__name__][lock_type]

    @classmethod
    def _require_lock(cls, lock_type: DbLockType) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦çœŸæ­£åŠ é”"""
        task_id = id(asyncio.current_task())
        return cls._current_locks.get(task_id) != lock_type

    @classmethod
    @contextlib.asynccontextmanager
    async def _lock_context(cls, lock_type: DbLockType):
        """å¸¦é‡å…¥æ£€æŸ¥çš„é”ä¸Šä¸‹æ–‡"""
        task_id = id(asyncio.current_task())
        need_lock = cls._require_lock(lock_type)

        if need_lock and (sem := cls.get_semaphore(lock_type)):
            cls._current_locks[task_id] = lock_type
            async with sem:
                yield
            cls._current_locks.pop(task_id, None)
        else:
            yield

    @classmethod
    async def create(
        cls, using_db: BaseDBAsyncClient | None = None, **kwargs: Any
    ) -> Self:
        """åˆ›å»ºæ•°æ®ï¼ˆä½¿ç”¨CREATEé”ï¼‰"""
        async with cls._lock_context(DbLockType.CREATE):
            # ç›´æ¥è°ƒç”¨çˆ¶ç±»çš„_createæ–¹æ³•é¿å…è§¦å‘saveçš„é”
            result = await super().create(using_db=using_db, **kwargs)
            if cache_type := cls.get_cache_type():
                await CacheRoot.invalidate_cache(cache_type, cls.get_cache_key(result))
            return result

    @classmethod
    async def get_or_create(
        cls,
        defaults: dict | None = None,
        using_db: BaseDBAsyncClient | None = None,
        **kwargs: Any,
    ) -> tuple[Self, bool]:
        """è·å–æˆ–åˆ›å»ºæ•°æ®ï¼ˆæ— é”ç‰ˆæœ¬ï¼Œä¾èµ–æ•°æ®åº“çº¦æŸï¼‰"""
        result = await super().get_or_create(
            defaults=defaults, using_db=using_db, **kwargs
        )
        if cache_type := cls.get_cache_type():
            await CacheRoot.invalidate_cache(cache_type, cls.get_cache_key(result[0]))
        return result

    @classmethod
    async def update_or_create(
        cls,
        defaults: dict | None = None,
        using_db: BaseDBAsyncClient | None = None,
        **kwargs: Any,
    ) -> tuple[Self, bool]:
        """æ›´æ–°æˆ–åˆ›å»ºæ•°æ®ï¼ˆä½¿ç”¨UPSERTé”ï¼‰"""
        async with cls._lock_context(DbLockType.UPSERT):
            try:
                # å…ˆå°è¯•æ›´æ–°ï¼ˆå¸¦è¡Œé”ï¼‰
                async with in_transaction():
                    if obj := await cls.filter(**kwargs).select_for_update().first():
                        await obj.update_from_dict(defaults or {})
                        await obj.save()
                        result = (obj, False)
                    else:
                        # åˆ›å»ºæ—¶ä¸é‡å¤åŠ é”
                        result = await cls.create(**kwargs, **(defaults or {})), True

                if cache_type := cls.get_cache_type():
                    await CacheRoot.invalidate_cache(
                        cache_type, cls.get_cache_key(result[0])
                    )
                return result
            except IntegrityError:
                # å¤„ç†æç«¯æƒ…å†µä¸‹çš„å”¯ä¸€çº¦æŸå†²çª
                obj = await cls.get(**kwargs)
                return obj, False

    async def save(
        self,
        using_db: BaseDBAsyncClient | None = None,
        update_fields: Iterable[str] | None = None,
        force_create: bool = False,
        force_update: bool = False,
    ):
        """ä¿å­˜æ•°æ®ï¼ˆæ ¹æ®æ“ä½œç±»å‹è‡ªåŠ¨é€‰æ‹©é”ï¼‰"""
        lock_type = (
            DbLockType.CREATE
            if getattr(self, "id", None) is None
            else DbLockType.UPDATE
        )
        async with self._lock_context(lock_type):
            await super().save(
                using_db=using_db,
                update_fields=update_fields,
                force_create=force_create,
                force_update=force_update,
            )
            if cache_type := getattr(self, "cache_type", None):
                await CacheRoot.invalidate_cache(
                    cache_type, self.__class__.get_cache_key(self)
                )

    async def delete(self, using_db: BaseDBAsyncClient | None = None):
        cache_type = getattr(self, "cache_type", None)
        key = self.__class__.get_cache_key(self) if cache_type else None
        # æ‰§è¡Œåˆ é™¤æ“ä½œ
        await super().delete(using_db=using_db)

        # æ¸…é™¤ç¼“å­˜
        if cache_type:
            await CacheRoot.invalidate_cache(cache_type, key)

    @classmethod
    async def safe_get_or_none(
        cls,
        *args,
        using_db: BaseDBAsyncClient | None = None,
        clean_duplicates: bool = True,
        **kwargs: Any,
    ) -> Self | None:
        """å®‰å…¨åœ°è·å–ä¸€æ¡è®°å½•æˆ–Noneï¼Œå¤„ç†å­˜åœ¨å¤šä¸ªè®°å½•æ—¶è¿”å›æœ€æ–°çš„é‚£ä¸ª
        æ³¨æ„ï¼Œé»˜è®¤ä¼šåˆ é™¤é‡å¤çš„è®°å½•ï¼Œä»…ä¿ç•™æœ€æ–°çš„

        å‚æ•°:
            *args: æŸ¥è¯¢å‚æ•°
            using_db: æ•°æ®åº“è¿æ¥
            clean_duplicates: æ˜¯å¦åˆ é™¤é‡å¤çš„è®°å½•ï¼Œä»…ä¿ç•™æœ€æ–°çš„
            **kwargs: æŸ¥è¯¢å‚æ•°

        è¿”å›:
            Self | None: æŸ¥è¯¢ç»“æœï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
        """
        try:
            # å…ˆå°è¯•ä½¿ç”¨ get_or_none è·å–å•ä¸ªè®°å½•
            try:
                return await with_db_timeout(
                    cls.get_or_none(*args, using_db=using_db, **kwargs),
                    operation=f"{cls.__name__}.get_or_none",
                )
            except MultipleObjectsReturned:
                # å¦‚æœå‡ºç°å¤šä¸ªè®°å½•çš„æƒ…å†µï¼Œè¿›è¡Œç‰¹æ®Šå¤„ç†
                logger.warning(
                    f"{cls.__name__} safe_get_or_none å‘ç°å¤šä¸ªè®°å½•: {kwargs}",
                    LOG_COMMAND,
                )

                # æŸ¥è¯¢æ‰€æœ‰åŒ¹é…è®°å½•
                records = await with_db_timeout(
                    cls.filter(*args, **kwargs).all(),
                    operation=f"{cls.__name__}.filter.all",
                )

                if not records:
                    return None

                # å¦‚æœéœ€è¦æ¸…ç†é‡å¤è®°å½•
                if clean_duplicates and hasattr(records[0], "id"):
                    # æŒ‰ id æ’åº
                    records = sorted(
                        records, key=lambda x: getattr(x, "id", 0), reverse=True
                    )
                    for record in records[1:]:
                        try:
                            await with_db_timeout(
                                record.delete(),
                                operation=f"{cls.__name__}.delete_duplicate",
                            )
                            logger.info(
                                f"{cls.__name__} åˆ é™¤é‡å¤è®°å½•:"
                                f" id={getattr(record, 'id', None)}",
                                LOG_COMMAND,
                            )
                        except Exception as del_e:
                            logger.error(f"åˆ é™¤é‡å¤è®°å½•å¤±è´¥: {del_e}")
                    return records[0]
                # å¦‚æœä¸éœ€è¦æ¸…ç†æˆ–æ²¡æœ‰ id å­—æ®µï¼Œåˆ™è¿”å›æœ€æ–°çš„è®°å½•
                if hasattr(cls, "id"):
                    return await with_db_timeout(
                        cls.filter(*args, **kwargs).order_by("-id").first(),
                        operation=f"{cls.__name__}.filter.order_by.first",
                    )
                # å¦‚æœæ²¡æœ‰ id å­—æ®µï¼Œåˆ™è¿”å›ç¬¬ä¸€ä¸ªè®°å½•
                return await with_db_timeout(
                    cls.filter(*args, **kwargs).first(),
                    operation=f"{cls.__name__}.filter.first",
                )
        except asyncio.TimeoutError:
            logger.error(
                f"æ•°æ®åº“æ“ä½œè¶…æ—¶: {cls.__name__}.safe_get_or_none", LOG_COMMAND
            )
            return None
        except Exception as e:
            # å…¶ä»–ç±»å‹çš„é”™è¯¯åˆ™ç»§ç»­æŠ›å‡º
            logger.error(
                f"æ•°æ®åº“æ“ä½œå¼‚å¸¸: {cls.__name__}.safe_get_or_none, {e!s}", LOG_COMMAND
            )
            raise


class DbUrlIsNode(HookPriorityException):
    """
    æ•°æ®åº“é“¾æ¥åœ°å€ä¸ºç©º
    """

    pass


class DbConnectError(Exception):
    """
    æ•°æ®åº“è¿æ¥é”™è¯¯
    """

    pass


POSTGRESQL_CONFIG = {
    "max_size": 30,  # æœ€å¤§è¿æ¥æ•°
    "min_size": 5,  # æœ€å°ä¿æŒçš„è¿æ¥æ•°ï¼ˆå¯é€‰ï¼‰
}


MYSQL_CONFIG = {
    "max_connections": 20,  # æœ€å¤§è¿æ¥æ•°
    "connect_timeout": 30,  # è¿æ¥è¶…æ—¶ï¼ˆå¯é€‰ï¼‰
}

SQLITE_CONFIG = {
    "journal_mode": "WAL",  # æé«˜å¹¶å‘å†™å…¥æ€§èƒ½
    "timeout": 30,  # é”ç­‰å¾…è¶…æ—¶ï¼ˆå¯é€‰ï¼‰
}


def get_config() -> dict:
    """è·å–æ•°æ®åº“é…ç½®"""
    parsed = urlparse(BotConfig.db_url)

    # åŸºç¡€é…ç½®
    config = {
        "connections": {
            "default": BotConfig.db_url  # é»˜è®¤ç›´æ¥ä½¿ç”¨è¿æ¥å­—ç¬¦ä¸²
        },
        "apps": {
            "models": {
                "models": MODELS,
                "default_connection": "default",
            }
        },
        "timezone": "Asia/Shanghai",
    }

    # æ ¹æ®æ•°æ®åº“ç±»å‹åº”ç”¨é«˜çº§é…ç½®
    if parsed.scheme.startswith("postgres"):
        config["connections"]["default"] = {
            "engine": "tortoise.backends.asyncpg",
            "credentials": {
                "host": parsed.hostname,
                "port": parsed.port or 5432,
                "user": parsed.username,
                "password": parsed.password,
                "database": parsed.path[1:],
            },
            **POSTGRESQL_CONFIG,
        }
    elif parsed.scheme == "mysql":
        config["connections"]["default"] = {
            "engine": "tortoise.backends.mysql",
            "credentials": {
                "host": parsed.hostname,
                "port": parsed.port or 3306,
                "user": parsed.username,
                "password": parsed.password,
                "database": parsed.path[1:],
            },
            **MYSQL_CONFIG,
        }
    elif parsed.scheme == "sqlite":
        config["connections"]["default"] = {
            "engine": "tortoise.backends.sqlite",
            "credentials": {
                "file_path": parsed.path or ":memory:",
            },
            **SQLITE_CONFIG,
        }
    return config


@PriorityLifecycle.on_startup(priority=1)
async def init():
    if not BotConfig.db_url:
        # raise DbUrlIsNode("æ•°æ®åº“é…ç½®ä¸ºç©ºï¼Œè¯·åœ¨.env.devä¸­é…ç½®DB_URL...")
        error = f"""
**********************************************************************
ğŸŒŸ **************************** é…ç½®ä¸ºç©º ************************* ğŸŒŸ
ğŸš€ è¯·æ‰“å¼€ WebUi è¿›è¡ŒåŸºç¡€é…ç½® ğŸš€
ğŸŒ é…ç½®åœ°å€ï¼šhttp://{driver.config.host}:{driver.config.port}/#/configure ğŸŒ
***********************************************************************
***********************************************************************
        """
        raise DbUrlIsNode("\n" + error.strip())
    try:
        await Tortoise.init(
            config=get_config(),
        )
        if SCRIPT_METHOD:
            db = Tortoise.get_connection("default")
            logger.debug(
                "å³å°†è¿è¡ŒSCRIPT_METHODæ–¹æ³•, åˆè®¡ "
                f"<u><y>{len(SCRIPT_METHOD)}</y></u> ä¸ª..."
            )
            sql_list = []
            for module, func in SCRIPT_METHOD:
                try:
                    sql = await func() if is_coroutine_callable(func) else func()
                    if sql:
                        sql_list += sql
                except Exception as e:
                    logger.debug(f"{module} æ‰§è¡ŒSCRIPT_METHODæ–¹æ³•å‡ºé”™...", e=e)
            for sql in sql_list:
                logger.debug(f"æ‰§è¡ŒSQL: {sql}")
                try:
                    await asyncio.wait_for(
                        db.execute_query_dict(sql), timeout=DB_TIMEOUT_SECONDS
                    )
                    # await TestSQL.raw(sql)
                except Exception as e:
                    logger.debug(f"æ‰§è¡ŒSQL: {sql} é”™è¯¯...", e=e)
            if sql_list:
                logger.debug("SCRIPT_METHODæ–¹æ³•æ‰§è¡Œå®Œæ¯•!")
        logger.debug("å¼€å§‹ç”Ÿæˆæ•°æ®åº“è¡¨ç»“æ„...")
        await Tortoise.generate_schemas()
        logger.debug("æ•°æ®åº“è¡¨ç»“æ„ç”Ÿæˆå®Œæ¯•!")
        logger.info("Database loaded successfully!")
    except Exception as e:
        raise DbConnectError(f"æ•°æ®åº“è¿æ¥é”™è¯¯... e:{e}") from e


async def disconnect():
    await connections.close_all()
